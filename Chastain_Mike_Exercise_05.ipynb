{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeChastain84/Mike_INFO5731_Fall2024/blob/main/Chastain_Mike_Exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O15W34Aop-v3",
        "outputId": "85d2efd8-1b86-4e7c-8d42-3db92bc81cea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ],
      "metadata": {
        "id": "loi8Sh7UE6ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "\n",
        "# Import all libraries, download necessary resources, and load DataFrames\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "# Specify file paths for the train and test datasets.\n",
        "train_file = '/content/drive/MyDrive/Colab Notebooks/INFO 5731/Week 13/stsa-train.txt'\n",
        "test_file = '/content/drive/MyDrive/Colab Notebooks/INFO 5731/Week 13/stsa-test.txt'\n",
        "\n",
        "# Define a function to load the data from the text files into a DataFrame\n",
        "# Each line in the text file is expected to have a label and review text\n",
        "def load_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            label, text = line[0], line[2:].strip()  # Extract label and text\n",
        "            data.append((int(label), text))\n",
        "    return pd.DataFrame(data, columns=['label', 'text'])\n",
        "\n",
        "# Load train and test datasets\n",
        "train_df = load_data(train_file)\n",
        "test_df = load_data(test_file)\n",
        "\n",
        "# Display dataframe samples:\n",
        "print('\\nTrain Data Sample:')\n",
        "print(train_df.head())\n",
        "print('\\nTest Data Sample:')\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x3LoZhnrbRF",
        "outputId": "79cd5cc3-622b-4c70-b02d-1330e3da7c02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Data Sample:\n",
            "   label                                               text\n",
            "0      1  a stirring , funny and finally transporting re...\n",
            "1      0  apparently reassembled from the cutting-room f...\n",
            "2      0  they presume their audience wo n't sit still f...\n",
            "3      1  this is a visually stunning rumination on love...\n",
            "4      1  jonathan parker 's bartleby should have been t...\n",
            "\n",
            "Test Data Sample:\n",
            "   label                                               text\n",
            "0      0     no movement , no yuks , not much of anything .\n",
            "1      0  a gob of drivel so sickly sweet , even the eag...\n",
            "2      0  gangs of new york is an unapologetic mess , wh...\n",
            "3      0  we never really feel involved with the story ,...\n",
            "4      1            this is one of polanski 's best films .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into 80% training and 20% validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_df['text'],   # Features: review texts\n",
        "    train_df['label'],  # Labels: sentiment labels (0 or 1)\n",
        "    test_size=0.2,      # Use 20% for validation\n",
        "    random_state=42     # Ensure reproducibility\n",
        ")\n",
        "\n",
        "# Display the sizes of the split datasets\n",
        "print(f'Training Data Size: {len(X_train)}')\n",
        "print(f'Validation Data Size: {len(X_val)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ-XLZRYt7OW",
        "outputId": "b1b96551-531d-44e5-df82-d81682316e44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Size: 5536\n",
            "Validation Data Size: 1384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "LM5wo-1gu78h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize PorterStemmer and stopwords\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a preprocessing function\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text by removing punctuation, lowercasing, and stemming.\"\"\"\n",
        "    text = re.sub(r'\\W+', ' ', str(text))  # Remove punctuation and numbers\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the training and validation sets\n",
        "X_train = X_train.apply(preprocess_text)\n",
        "X_val = X_val.apply(preprocess_text)\n",
        "\n",
        "# Create a Pipeline for TF-IDF and Multinomial Naive Bayes\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', min_df=5)),  # TF-IDF Vectorization\n",
        "    ('clf', MultinomialNB()),  # Multinomial Naive Bayes Classifier\n",
        "])\n",
        "\n",
        "# Perform 10 fold cross validation:\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=10, scoring='accuracy')\n",
        "print(f'Cross-Validation Scores: {cv_scores}')\n",
        "print(f'Mean Accuracy: {cv_scores.mean()}')\n",
        "\n",
        "# Train the Model on Training Data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the Model on the Validation Data\n",
        "y_val_pred = pipeline.predict(X_val)\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(metrics.classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Print validation accuracy\n",
        "print(f\"Validation Accuracy: {metrics.accuracy_score(y_val, y_val_pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjGfangCxSV_",
        "outputId": "44e9b7d5-0321-465c-b6e6-db835cc92f54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Scores: [0.74729242 0.77797834 0.73285199 0.77797834 0.77436823 0.76714801\n",
            " 0.7721519  0.77034358 0.76130199 0.78661844]\n",
            "Mean Accuracy: 0.7668033241720579\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.69      0.74       671\n",
            "           1       0.74      0.82      0.78       713\n",
            "\n",
            "    accuracy                           0.76      1384\n",
            "   macro avg       0.76      0.76      0.76      1384\n",
            "weighted avg       0.76      0.76      0.76      1384\n",
            "\n",
            "Validation Accuracy: 0.7608381502890174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ],
      "metadata": {
        "id": "Z7UkOH3Mu2oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# SVM Implementation\n",
        "# Create a pipeline for TF-IDF and SVM\n",
        "svm_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', min_df=5)),  # TF-IDF Vectorization\n",
        "    ('clf', SVC(kernel='linear')),  # Support Vector Machine with linear kernel\n",
        "])\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "svm_cv_scores = cross_val_score(svm_pipeline, X_train, y_train, cv=10, scoring='accuracy')\n",
        "print(f'SVM Cross-Validation Scores: {svm_cv_scores}')\n",
        "print(f'SVM Mean Accuracy: {svm_cv_scores.mean()}')\n",
        "\n",
        "# Train the SVM model\n",
        "svm_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the SVM model on the validation set\n",
        "y_val_pred_svm = svm_pipeline.predict(X_val)\n",
        "\n",
        "# Print classification report and accuracy for SVM\n",
        "print(\"\\nSVM Classification Report:\")\n",
        "print(metrics.classification_report(y_val, y_val_pred_svm))\n",
        "print(f\"SVM Validation Accuracy: {metrics.accuracy_score(y_val, y_val_pred_svm)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVpds20pGZZG",
        "outputId": "bac1c93c-717e-4f1b-8bf0-780a493a7280"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Cross-Validation Scores: [0.73646209 0.76714801 0.73104693 0.76353791 0.77617329 0.74548736\n",
            " 0.75949367 0.73417722 0.74864376 0.77757685]\n",
            "SVM Mean Accuracy: 0.7539747096572029\n",
            "\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.72      0.74       671\n",
            "           1       0.75      0.79      0.77       713\n",
            "\n",
            "    accuracy                           0.75      1384\n",
            "   macro avg       0.75      0.75      0.75      1384\n",
            "weighted avg       0.75      0.75      0.75      1384\n",
            "\n",
            "SVM Validation Accuracy: 0.7543352601156069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN"
      ],
      "metadata": {
        "id": "GHuJfQnhvKjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# KNN Implementation:\n",
        "# Create a pipeline for TF-IDF and KNN\n",
        "knn_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', min_df=5)),  # TF-IDF Vectorization\n",
        "    ('clf', KNeighborsClassifier(n_neighbors=5)),  # KNN with 5 neighbors\n",
        "])\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "knn_cv_scores = cross_val_score(knn_pipeline, X_train, y_train, cv=10, scoring='accuracy')\n",
        "print(f'KNN Cross-Validation Scores: {knn_cv_scores}')\n",
        "print(f'KNN Mean Accuracy: {knn_cv_scores.mean()}')\n",
        "\n",
        "# Train the KNN model\n",
        "knn_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the KNN model on the validation set\n",
        "y_val_pred_knn = knn_pipeline.predict(X_val)\n",
        "\n",
        "# Print classification report and accuracy for KNN\n",
        "print(\"\\nKNN Classification Report:\")\n",
        "print(metrics.classification_report(y_val, y_val_pred_knn))\n",
        "print(f\"KNN Validation Accuracy: {metrics.accuracy_score(y_val, y_val_pred_knn)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHpb67BJHf81",
        "outputId": "fda3817a-650e-4b3d-b809-8468f3e9ab6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Cross-Validation Scores: [0.56498195 0.55054152 0.49458484 0.54873646 0.52707581 0.5\n",
            " 0.51356239 0.4954792  0.53164557 0.5443038 ]\n",
            "KNN Mean Accuracy: 0.527091153602601\n",
            "\n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.61      0.55       671\n",
            "           1       0.54      0.44      0.48       713\n",
            "\n",
            "    accuracy                           0.52      1384\n",
            "   macro avg       0.52      0.52      0.52      1384\n",
            "weighted avg       0.52      0.52      0.52      1384\n",
            "\n",
            "KNN Validation Accuracy: 0.5202312138728323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree"
      ],
      "metadata": {
        "id": "3wqkwHPPwC7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Decision Tree Implementation:\n",
        "# Create a pipeline for TF-IDF and Decision Tree\n",
        "dt_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', min_df=5)),  # TF-IDF Vectorization\n",
        "    ('clf', DecisionTreeClassifier(random_state=42)),  # Decision Tree Classifier\n",
        "])\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "dt_cv_scores = cross_val_score(dt_pipeline, X_train, y_train, cv=10, scoring='accuracy')\n",
        "print(f'Decision Tree Cross-Validation Scores: {dt_cv_scores}')\n",
        "print(f'Decision Tree Mean Accuracy: {dt_cv_scores.mean()}')\n",
        "\n",
        "# Train the Decision Tree model and evaluate it on the validation set\n",
        "dt_pipeline.fit(X_train, y_train)\n",
        "y_val_pred_dt = dt_pipeline.predict(X_val)\n",
        "\n",
        "# Print classification report and accuracy for Decision Tree\n",
        "print(\"\\nDecision Tree Classification Report:\")\n",
        "print(metrics.classification_report(y_val, y_val_pred_dt))\n",
        "print(f\"Decision Tree Validation Accuracy: {metrics.accuracy_score(y_val, y_val_pred_dt)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZZlC5LqkOEK",
        "outputId": "a735b68b-e8af-4112-f4e8-435b84a4634a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Cross-Validation Scores: [0.64259928 0.70938628 0.6732852  0.64981949 0.6732852  0.6534296\n",
            " 0.62567812 0.62929476 0.62748644 0.64737794]\n",
            "Decision Tree Mean Accuracy: 0.6531642305507863\n",
            "\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.63      0.64       671\n",
            "           1       0.66      0.69      0.67       713\n",
            "\n",
            "    accuracy                           0.66      1384\n",
            "   macro avg       0.66      0.66      0.66      1384\n",
            "weighted avg       0.66      0.66      0.66      1384\n",
            "\n",
            "Decision Tree Validation Accuracy: 0.6567919075144508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "LAAqxlN6x1md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import RFC_4122\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Random Forest Implementation:\n",
        "# Create a pipeline for TF-IDF and Random Forest\n",
        "rf_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', min_df=5)),  # TF-IDF Vectorization\n",
        "    ('clf', RandomForestClassifier(n_estimators=100, random_state=42)),  # Random Forest Classifier\n",
        "])\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "rf_cv_scores = cross_val_score(rf_pipeline, X_train, y_train, cv=10, scoring='accuracy')\n",
        "print(f'Random Forest Cross-Validation Scores: {rf_cv_scores}')\n",
        "print(f'Random Forest Mean Accuracy: {rf_cv_scores.mean()}')\n",
        "\n",
        "# Train the Random Forest model and evaluate it on the validation set\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "y_val_pred_rf = rf_pipeline.predict(X_val)\n",
        "\n",
        "print(\"\\nRandom Forest Classification Report:\")\n",
        "print(metrics.classification_report(y_val, y_val_pred_rf))\n",
        "print(f\"Random Forest Validation Accuracy: {metrics.accuracy_score(y_val, y_val_pred_rf)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yla59jCxs3T",
        "outputId": "3a1194c8-4de7-420c-c108-3b0c47d4f70e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Cross-Validation Scores: [0.70216606 0.72563177 0.72382671 0.70577617 0.74187726 0.71119134\n",
            " 0.71066908 0.6835443  0.67631103 0.72332731]\n",
            "Random Forest Mean Accuracy: 0.7104321031981773\n",
            "\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.69      0.71       671\n",
            "           1       0.72      0.75      0.74       713\n",
            "\n",
            "    accuracy                           0.72      1384\n",
            "   macro avg       0.72      0.72      0.72      1384\n",
            "weighted avg       0.72      0.72      0.72      1384\n",
            "\n",
            "Random Forest Validation Accuracy: 0.7247109826589595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost"
      ],
      "metadata": {
        "id": "5mj2z00M7z3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# XGBoost Implementation:\n",
        "# Create a pipeline for TF-IDF and XGBoost\n",
        "xgb_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', min_df=5)),  # TF-IDF Vectorization\n",
        "    ('clf', XGBClassifier(eval_metric='logloss', random_state=42)),  # XGBoost Classifier\n",
        "])\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "xgb_cv_scores = cross_val_score(xgb_pipeline, X_train, y_train, cv=10, scoring='accuracy')\n",
        "print(f'XGBoost Cross-Validation Scores: {xgb_cv_scores}')\n",
        "print(f'XGBoost Mean Accuracy: {xgb_cv_scores.mean()}')\n",
        "\n",
        "# Train the XGBoost model and evaluate it on the validation set\n",
        "xgb_pipeline.fit(X_train, y_train)\n",
        "y_val_pred_xgb = xgb_pipeline.predict(X_val)\n",
        "\n",
        "# Print classification report and accuracy for XGBoost\n",
        "print(\"\\nXGBoost Classification Report:\")\n",
        "print(metrics.classification_report(y_val, y_val_pred_xgb))\n",
        "print(f\"XGBoost Validation Accuracy: {metrics.accuracy_score(y_val, y_val_pred_xgb)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66DSw7lY744Z",
        "outputId": "f3d010eb-d17a-4e45-d977-45bd1bfb9194"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Cross-Validation Scores: [0.67509025 0.71841155 0.68231047 0.67870036 0.70036101 0.69133574\n",
            " 0.69077758 0.67631103 0.68173599 0.69620253]\n",
            "XGBoost Mean Accuracy: 0.689123651105555\n",
            "\n",
            "XGBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.71      0.70       671\n",
            "           1       0.72      0.70      0.71       713\n",
            "\n",
            "    accuracy                           0.70      1384\n",
            "   macro avg       0.70      0.70      0.70      1384\n",
            "weighted avg       0.70      0.70      0.70      1384\n",
            "\n",
            "XGBoost Validation Accuracy: 0.703757225433526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "bzo4mUV7Bn5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Tokenize the training and validation data:\n",
        "X_train_tokenized = [review.split() for review in X_train]\n",
        "X_val_tokenized = [review.split() for review in X_val]\n",
        "\n",
        "# Train Word2Vec on the training data\n",
        "word2vec_model = Word2Vec(sentences=X_train_tokenized, vector_size=100, window=5, min_count=2, workers=4, sg=1)\n",
        "\n",
        "# Compute the mean vector for each review\n",
        "def compute_mean_vector(text, model, vector_size):\n",
        "    \"\"\"Compute the mean word vector for a given review.\"\"\"\n",
        "    vectors = [model.wv[word] for word in text if word in model.wv]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "# Compute mean vectors for training and validation sets\n",
        "X_train_vectors = np.array([compute_mean_vector(tokens, word2vec_model, 100) for tokens in X_train_tokenized])\n",
        "X_val_vectors = np.array([compute_mean_vector(tokens, word2vec_model, 100) for tokens in X_val_tokenized])\n",
        "\n",
        "# Train and perform 10-fold cross-validation on Random Forest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "rf_cv_scores_word2vec = cross_val_score(\n",
        "    rf_classifier,\n",
        "    X_train_vectors,\n",
        "    y_train,\n",
        "    cv=10,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Print cross-validation results\n",
        "print(f\"Word2Vec + Random Forest Cross-Validation Scores: {rf_cv_scores_word2vec}\")\n",
        "print(f\"Word2Vec + Random Forest Mean Accuracy: {rf_cv_scores_word2vec.mean()}\")\n",
        "\n",
        "# Train the Random Forest classifier on the full training set\n",
        "rf_classifier.fit(X_train_vectors, y_train)\n",
        "\n",
        "# Evaluate the classifier on the validation set\n",
        "y_val_pred_rf_word2vec = rf_classifier.predict(X_val_vectors)\n",
        "\n",
        "# Print classification report and accuracy\n",
        "print(\"\\nWord2Vec + Random Forest Classification Report:\")\n",
        "print(classification_report(y_val, y_val_pred_rf_word2vec))\n",
        "print(f\"Validation Accuracy: {accuracy_score(y_val, y_val_pred_rf_word2vec)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFeCuNCJBq_Q",
        "outputId": "fb11a4f2-9fbd-4f80-9e6a-e75898924013"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec + Random Forest Cross-Validation Scores: [0.63357401 0.58303249 0.59205776 0.63537906 0.6299639  0.57039711\n",
            " 0.63110307 0.61482821 0.56781193 0.59312839]\n",
            "Word2Vec + Random Forest Mean Accuracy: 0.6051275941533218\n",
            "\n",
            "Word2Vec + Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.51      0.56       671\n",
            "           1       0.60      0.70      0.65       713\n",
            "\n",
            "    accuracy                           0.61      1384\n",
            "   macro avg       0.61      0.61      0.60      1384\n",
            "weighted avg       0.61      0.61      0.61      1384\n",
            "\n",
            "Validation Accuracy: 0.6091040462427746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT 1"
      ],
      "metadata": {
        "id": "2G9tWPRaE7xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "# from torch.optim import AdamW\n",
        "# from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "# from tqdm import tqdm\n",
        "# import numpy as np\n",
        "# import torch\n",
        "\n",
        "# # Define the custom dataset class (unchanged)\n",
        "# class TextDataset(Dataset):\n",
        "#     def __init__(self, texts, labels, tokenizer, max_len):\n",
        "#         self.texts = texts\n",
        "#         self.labels = labels\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_len = max_len\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.texts)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         text = str(self.texts[idx])\n",
        "#         label = self.labels[idx]\n",
        "#         encoding = self.tokenizer.encode_plus(\n",
        "#             text,\n",
        "#             add_special_tokens=True,\n",
        "#             max_length=self.max_len,\n",
        "#             return_token_type_ids=False,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             return_attention_mask=True,\n",
        "#             return_tensors=\"pt\",\n",
        "#         )\n",
        "#         return {\n",
        "#             \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "#             \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "#             \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "#         }\n",
        "\n",
        "# # Initialize tokenizer and device\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Define configuration for BERT\n",
        "# config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "# config.hidden_dropout_prob = 0.3\n",
        "# config.attention_probs_dropout_prob = 0.3\n",
        "\n",
        "# # Define hyperparameters\n",
        "# max_len = 128\n",
        "# batch_size = 16\n",
        "# epochs = 3\n",
        "\n",
        "# # K-Fold Cross-Validation\n",
        "# kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "# fold_results = []\n",
        "\n",
        "# for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "#     print(f\"\\n*** Fold {fold + 1} ***\")\n",
        "\n",
        "#     # Split data for this fold\n",
        "#     train_texts_fold = [X_train[i] for i in train_idx]\n",
        "#     val_texts_fold = [X_train[i] for i in val_idx]\n",
        "#     train_labels_fold = [y_train[i] for i in train_idx]\n",
        "#     val_labels_fold = [y_train[i] for i in val_idx]\n",
        "\n",
        "#     # Create datasets and dataloaders\n",
        "#     train_dataset = TextDataset(train_texts_fold, train_labels_fold, tokenizer, max_len)\n",
        "#     val_dataset = TextDataset(val_texts_fold, val_labels_fold, tokenizer, max_len)\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "#     # Initialize model\n",
        "#     model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config).to(device)\n",
        "\n",
        "#     # Initialize optimizer and scheduler\n",
        "#     optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
        "#     total_steps = len(train_loader) * epochs\n",
        "#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "#     # Train the model for this fold\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#             loss = outputs.loss\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             scheduler.step()\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#     # Evaluate the model on the validation fold\n",
        "#     model.eval()\n",
        "#     predictions, true_labels = [], []\n",
        "#     with torch.no_grad():\n",
        "#         for batch in val_loader:\n",
        "#             input_ids = batch[\"input_ids\"].to(device)\n",
        "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
        "#             labels = batch[\"labels\"].to(device)\n",
        "\n",
        "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#             logits = outputs.logits\n",
        "#             predictions.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
        "#             true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "#     # Calculate metrics for this fold\n",
        "#     accuracy = accuracy_score(true_labels, predictions)\n",
        "#     f1 = f1_score(true_labels, predictions, average=\"weighted\")\n",
        "#     precision = precision_score(true_labels, predictions, average=\"weighted\")\n",
        "#     print(f\"Fold {fold + 1} - Accuracy: {accuracy}, F1: {f1}, Precision: {precision}\")\n",
        "#     fold_results.append((accuracy, f1, precision))\n",
        "\n",
        "# # Aggregate results across folds\n",
        "# fold_accuracies = [result[0] for result in fold_results]\n",
        "# fold_f1_scores = [result[1] for result in fold_results]\n",
        "# fold_precisions = [result[2] for result in fold_results]\n",
        "\n",
        "# print(\"\\n*** Cross-Validation Results ***\")\n",
        "# print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f}\")\n",
        "# print(f\"Mean F1-Score: {np.mean(fold_f1_scores):.4f}\")\n",
        "# print(f\"Mean Precision: {np.mean(fold_precisions):.4f}\")\n"
      ],
      "metadata": {
        "id": "w-jj_G0oE_Rq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT with 10-fold cross-validation"
      ],
      "metadata": {
        "id": "AIVsDCwEOoPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Define a custom dataset class to prepare the text and labels for BERT\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):  #Sets the class up with necessary data and tools\n",
        "        self.texts = texts            # List of text samples\n",
        "        self.labels = labels          # List of labels corresponding to the texts (0 for negative, 1 for positive)\n",
        "        self.tokenizer = tokenizer    # BERT tokenizer to preprocess the text\n",
        "        self.max_len = max_len        # Maximum sequence length for padding/truncation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)        # Returns the total number of samples in the dataset\n",
        "\n",
        "    def __getitem__(self, idx):       # Fetches a sample (text and its label) based on the index and processes it\n",
        "        text = str(self.texts[idx])   # Converts the text to a str\n",
        "        label = self.labels[idx]      # Fetches the corresponding label\n",
        "\n",
        "        # Uses the tokenizer to preprocess the text:\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        # Returns the processed input and label:\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Define helper functions\n",
        "# The dataloader function creates batches of data for training and evaluating:\n",
        "def create_dataloader(texts, labels, tokenizer, max_len, batch_size, shuffle=False):\n",
        "    \"\"\"Create DataLoader for a dataset.\"\"\"\n",
        "    dataset = TextDataset(texts, labels, tokenizer, max_len)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "# The train_model function...\n",
        "def train_model(model, data_loader, optimizer, scheduler, device):\n",
        "    \"\"\"Train the model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    \"\"\"Evaluate the model.\"\"\"\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "    return true_labels, predictions\n",
        "\n",
        "# Initialize tokenizer and device\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Perform 10-Fold Cross-Validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "max_len = 128\n",
        "batch_size = 16\n",
        "epochs = 1    # I removed this from 3 to 1 epoch. It was taking way too long to run with 3 epochs.\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "    print(f\"\\n*** Fold {fold + 1} ***\")\n",
        "\n",
        "    # Split data for this fold using `.iloc`\n",
        "    train_texts_fold = X_train.iloc[train_idx].tolist()\n",
        "    val_texts_fold = X_train.iloc[val_idx].tolist()\n",
        "    train_labels_fold = y_train.iloc[train_idx].tolist()\n",
        "    val_labels_fold = y_train.iloc[val_idx].tolist()\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = create_dataloader(train_texts_fold, train_labels_fold, tokenizer, max_len, batch_size, shuffle=True)\n",
        "    val_loader = create_dataloader(val_texts_fold, val_labels_fold, tokenizer, max_len, batch_size)\n",
        "\n",
        "    # Initialize BERT model for this fold\n",
        "    config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "    config.hidden_dropout_prob = 0.3\n",
        "    config.attention_probs_dropout_prob = 0.3\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config).to(device)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Train and evaluate the model for this fold\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        train_model(model, train_loader, optimizer, scheduler, device)\n",
        "\n",
        "    # Evaluate the model\n",
        "    true_labels, predictions = evaluate_model(model, val_loader, device)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions, average=\"weighted\")\n",
        "    precision = precision_score(true_labels, predictions, average=\"weighted\")\n",
        "    print(f\"Fold {fold + 1} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}\")\n",
        "    fold_results.append((accuracy, f1, precision))\n",
        "\n",
        "# Aggregate Cross-Validation Results\n",
        "fold_accuracies = [result[0] for result in fold_results]\n",
        "fold_f1_scores = [result[1] for result in fold_results]\n",
        "fold_precisions = [result[2] for result in fold_results]\n",
        "\n",
        "print(\"\\n*** Cross-Validation Results ***\")\n",
        "print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f}\")\n",
        "print(f\"Mean F1-Score: {np.mean(fold_f1_scores):.4f}\")\n",
        "print(f\"Mean Precision: {np.mean(fold_precisions):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "mIMeza-6On1I",
        "outputId": "2ef7128c-15a2-4378-ac7c-cebede44a493"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** Fold 1 ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 312/312 [2:06:41<00:00, 24.36s/it]\n",
            "Evaluating: 100%|██████████| 35/35 [04:04<00:00,  7.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 - Accuracy: 0.6426, F1: 0.6413, Precision: 0.6428\n",
            "\n",
            "*** Fold 2 ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   6%|▌         | 19/312 [07:17<1:52:23, 23.02s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d7304c214602>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-d7304c214602>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1668\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1669\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PLEASE READ BELOW FOR QUESTION 1 BERT WITH 10-FOLD CROSS-VALIDATION:\n",
        "\n",
        "I chose to use a KeyboardInterrupt on the above code because it is taking too long to compute one fold.\n",
        "If I had access to a more powerful processor I would have let it run further. This is the second time I've run it.\n",
        "I started with 3 Epochs and that was taking much longer. I chose to use 1 Epoch so I could get through 1 fold in a reasonable time.\n",
        "\n",
        "I've tried using the GPU Hardware Accelerator, but I receive this message:\n",
        "\n",
        "Cannot connect to GPU backend:\n",
        "You cannot currently connect to a GPU due to usage limits in Colab. Learn more\n",
        "To get more access to GPUs, consider purchasing Colab compute units with Pay As You Go.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "u-iBo0OiKGs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-means"
      ],
      "metadata": {
        "id": "-n-D0Gjn7oGl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab68731-76b7-41f4-e11b-b4c0cf23e6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Sample:\n",
            "                                        Product Name Brand Name   Price  \\\n",
            "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "\n",
            "   Rating                                            Reviews  Review Votes  \n",
            "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
            "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
            "2       5                                       Very pleased           0.0  \n",
            "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
            "4       4  Great phone to replace my lost phone. The only...           0.0  \n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import re\n",
        "\n",
        "# Step 1: Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/INFO 5731/Week 13/Amazon_Unlocked_Mobile.csv')\n",
        "print(\"Dataset Sample:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Explore the data\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn1CV7B2zT0J",
        "outputId": "1a5cc8d5-1149-418b-d22e-44813a5bf38c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 413840 entries, 0 to 413839\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count   Dtype  \n",
            "---  ------        --------------   -----  \n",
            " 0   Product Name  413840 non-null  object \n",
            " 1   Brand Name    348669 non-null  object \n",
            " 2   Price         407907 non-null  float64\n",
            " 3   Rating        413840 non-null  int64  \n",
            " 4   Reviews       413770 non-null  object \n",
            " 5   Review Votes  401544 non-null  float64\n",
            "dtypes: float64(2), int64(1), object(3)\n",
            "memory usage: 18.9+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data preprocessing\n",
        "\n",
        "df = df.dropna(subset=['Reviews'])  # Remove rows with NaN reviews\n",
        "df['Cleaned_Reviews'] = df['Reviews'].str.lower()  # Convert to lowercase\n",
        "df['Cleaned_Reviews'] = df['Cleaned_Reviews'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))  # Remove special characters\n",
        "\n",
        "# Step 4: Vectorize the text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')  # Let TF-IDF handle stop word removal\n",
        "features = vectorizer.fit_transform(df['Cleaned_Reviews'])\n",
        "\n",
        "print(f\"\\nTF-IDF Matrix Shape: {features.shape}\")  # Verify dimensions of the matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkjhiTGty0xU",
        "outputId": "d8bfeab2-dac9-43ba-88df-8ae6df0fae7b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix Shape: (413770, 115273)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Apply K-Means Clustering\n",
        "print(\"\\nApplying K-Means Clustering...\")\n",
        "kmeans = KMeans(n_clusters=5, max_iter=100, n_init=5, random_state=42)  # Using 5 clusters\n",
        "kmeans.fit(features)\n",
        "\n",
        "# Add the cluster labels to the DataFrame\n",
        "df['KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "# # Step 6: Evaluate Clustering with Silhouette Score OMMITTED TO SAVE TIME\n",
        "# silhouette_avg = silhouette_score(features, kmeans.labels_)\n",
        "# print(f\"\\nK-Means Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "# Display the first few rows of the DataFrame with cluster labels\n",
        "print(\"\\nClustered Data Sample:\")\n",
        "print(df[['Reviews', 'KMeans_Cluster']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "2rsIXHYc7Ya3",
        "outputId": "d81a7567-eefe-4c77-a28d-cfc2d78cd7ef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applying K-Means Clustering...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7291cec18682>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nApplying K-Means Clustering...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Using 5 clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Add the cluster labels to the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m             \u001b[0;31m# run a k-means once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m             labels, inertia, centers, n_iter_ = kmeans_single(\n\u001b[0m\u001b[1;32m   1520\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mcontroller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_threadpool_controller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_api\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, sample_weight, centers_init, max_iter, verbose, tol, n_threads)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m         lloyd_iter(\n\u001b[0m\u001b[1;32m    708\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_k_means_lloyd.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_lloyd.lloyd_iter_chunked_sparse\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_k_means_common.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_common._relocate_empty_clusters_sparse\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_from_c_func_and_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "I stopped the code above with a keyboard interrupt. K-means has been running for 2+ hours and it's 9:55. I never expected that.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dkzevubnW4f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN"
      ],
      "metadata": {
        "id": "1oyxAdEL7wlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import TruncatedSVD      # To assist with computational requirements\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Vectorize the Text Data\n",
        "print(\"\\nVectorizing the text data with TF-IDF...\")\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "features = vectorizer.fit_transform(df['Reviews'])\n",
        "print(f\"TF-IDF Matrix Shape: {features.shape}\")\n",
        "\n",
        "# Reduce Dimensionality with TruncatedSVD\n",
        "print(\"\\nReducing dimensionality with TruncatedSVD...\")\n",
        "svd = TruncatedSVD(n_components=300, random_state=42)\n",
        "reduced_features = svd.fit_transform(features)\n",
        "print(f\"Reduced TF-IDF Shape: {reduced_features.shape}\")\n",
        "\n",
        "# Step 1: Apply DBSCAN without parameter optimization\n",
        "# DBSCAN without any parameter optimization and see the results.\n",
        "print(\"\\nApplying initial DBSCAN...\")\n",
        "dbscan = DBSCAN(eps=2.3, min_samples=2, metric='euclidean', n_jobs=-1)\n",
        "model = dbscan.fit(reduced_features)\n",
        "\n",
        "# Add cluster labels to the DataFrame\n",
        "df['DBSCAN_Labels'] = model.labels_\n",
        "\n",
        "# Print unique labels\n",
        "print(f\"Unique labels from initial DBSCAN: {np.unique(df['DBSCAN_Labels'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKF5KMyK8E3v",
        "outputId": "63e65882-caf2-41a0-d613-ab335e38c2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vectorizing the text data with TF-IDF...\n",
            "TF-IDF Matrix Shape: (413770, 68241)\n",
            "\n",
            "Reducing dimensionality with TruncatedSVD...\n",
            "Reduced TF-IDF Shape: (413770, 300)\n",
            "\n",
            "Applying initial DBSCAN...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **\"Your session crashed after using all available RAM.\"**\n",
        "\n",
        "## Everytime I run DBSCAN I receive the message above. I would need more powerful hardware or access to GPUs in colab to complete the assignment."
      ],
      "metadata": {
        "id": "IEX47ktOii6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create K-distance graph to determine optimal 'eps'\n",
        "print(\"\\nCreating K-Distance Graph...\")\n",
        "sample_size = 10000  # Use a smaller subset for K-distance graph to save time\n",
        "sampled_features = reduced_features[:sample_size]\n",
        "\n",
        "neigh = NearestNeighbors(n_neighbors=2)\n",
        "nbrs = neigh.fit(sampled_features)\n",
        "distances, indices = nbrs.kneighbors(sampled_features)\n",
        "\n",
        "# Sort distances for the K-distance graph\n",
        "distances = np.sort(distances, axis=0)\n",
        "distances = distances[:, 1]\n",
        "\n",
        "# Plot the K-Distance graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(distances)\n",
        "plt.title('K-Distance Graph - Determine Optimal Epsilon', fontsize=16)\n",
        "plt.xlabel('Data Points (sorted by Distance)', fontsize=12)\n",
        "plt.ylabel('Epsilon (Distance)', fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pro6ao3bAS1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Apply DBSCAN with optimized 'eps'\n",
        "print(\"\\nApplying DBSCAN with optimized parameters...\")\n",
        "dbscan = DBSCAN(eps=1.3, min_samples=4, metric='euclidean', n_jobs=-1)  # Adjusted parameters\n",
        "model = dbscan.fit(reduced_features)\n",
        "\n",
        "# Add the new cluster labels to the DataFrame\n",
        "df['DBSCAN_Labels_Optimized'] = model.labels_\n",
        "\n",
        "# Print unique labels after optimization\n",
        "print(f\"Unique labels after optimized DBSCAN: {np.unique(df['DBSCAN_Labels_Optimized'])}\")\n",
        "\n",
        "# Display a sample of the clustered data\n",
        "print(\"\\nClustered Data Sample (with Optimized DBSCAN):\")\n",
        "print(df[['Reviews', 'DBSCAN_Labels_Optimized']].head())"
      ],
      "metadata": {
        "id": "1JELELVUAlwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hierarchical clustering"
      ],
      "metadata": {
        "id": "EdRnCgJu7xAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 0: Load and preprocess the data\n",
        "print(\"\\nLoading and preprocessing data...\")\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/INFO 5731/Week 13/Amazon_Unlocked_Mobile.csv')\n",
        "df = df.dropna(subset=['Reviews'])  # Drop rows with missing reviews\n",
        "\n",
        "# Vectorize text using TF-IDF\n",
        "print(\"\\nVectorizing text data with TF-IDF...\")\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "features = vectorizer.fit_transform(df['Reviews'])\n",
        "print(f\"TF-IDF Matrix Shape: {features.shape}\")\n",
        "\n",
        "# Step 1: Define Necessary Functions\n",
        "def ward_hierarchical_clustering(feature_matrix):\n",
        "    \"\"\"Perform Ward hierarchical clustering on the given feature matrix.\"\"\"\n",
        "    cosine_distance = 1 - cosine_similarity(feature_matrix)\n",
        "    linkage_matrix = ward(cosine_distance)\n",
        "    return linkage_matrix\n",
        "\n",
        "def plot_hierarchical_clusters(linkage_matrix, data, p=30, figure_size=(12, 10)):\n",
        "    \"\"\"Plot hierarchical clusters as a dendrogram.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=figure_size)\n",
        "\n",
        "    # Generate the dendrogram\n",
        "    R = dendrogram(\n",
        "        linkage_matrix,\n",
        "        orientation=\"left\",\n",
        "        truncate_mode=\"lastp\",  # Show only the last 'p' merged clusters\n",
        "        p=p,\n",
        "        leaf_font_size=10,\n",
        "    )\n",
        "    plt.title(\"Hierarchical Clustering Dendrogram\", fontsize=16)\n",
        "    plt.xlabel(\"Distance\", fontsize=12)\n",
        "    plt.ylabel(\"Clusters\", fontsize=12)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2TWglK_8FlH",
        "outputId": "2a32fef2-6324-4a9b-dc94-be9f5c874afb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading and preprocessing data...\n",
            "\n",
            "Vectorizing text data with TF-IDF...\n",
            "TF-IDF Matrix Shape: (413770, 68241)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Perform Ward Hierarchical Clustering\n",
        "print(\"\\nPerforming Ward Hierarchical Clustering...\")\n",
        "linkage_matrix = ward_hierarchical_clustering(features)\n",
        "\n",
        "# Step 3: Visualize the Dendrogram\n",
        "print(\"\\nPlotting the Dendrogram...\")\n",
        "plot_hierarchical_clusters(linkage_matrix, df, p=30, figure_size=(12, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoOk3KNIGFCg",
        "outputId": "b2b85153-557a-4e66-89ba-90891f658821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing Ward Hierarchical Clustering...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below is a picture of the error I get when I run this code. It continuously crashes due to limitations in Google colab."
      ],
      "metadata": {
        "id": "Alm9zAoKmuKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![session crashed.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgwAAAA1CAYAAAAtbLNDAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAACEiSURBVHhe7Z0HfBXF+rBfamgBQu+9hSJVRJoICChWhCuCIuj9FKRcC17LBUXE714bWAFRAQWlCipFRDoI0nuv0hMgQAhJ6P993uwelsNpCQHRzMNvOGdnZ2dnZnffNrMn6S5cuHDJQhzc3x2883yVSQ3Onz8vGTJkkHTp0tk5105CQoIMHDhQIiMjpU2bNjJv3jxNefPmlaxZs8pTTz0ly5Ytk4kTJ8orr7wi2bJl0/LNmzeX2267TetYvHixTJ48WffHxMTIN998I88995zWAYMHD9bPbt26ydChQyVTpkxaL/2Ij4+XDz74QFq0aKH1UbZQoULaFti2bZsMGzZMnnnmGSlfvrynrmeffVbH2bs+8r766is5d+6cdO3aVdt1+PBhLe9m+PDhsmfPHsmYMaP07NlTIiIi7D2XWbBggUyfPl1efPFFyZ8/v50rMmnSJFm/fr307t1bEhMTZdCgQdKpUyepUKGCZ//mzZvlhRdekOXLl8tPP/2kZekXXLx4Udt69uxZradUqVKSL18+3Tdt2jRZs2aNHrt//375/PPPPX0/duyY/O9//5O2bdtK3bp1k9139/HOtTMYUhPuQUtm6nO1Y8cOKVeunL0ndeG5M6QCu47J/JpnJWJ1YcnxzgHZMTYpO+ytCLmjV7akDYujHx+SVX0v2lsW7cOlxec57Q3v/emlkFXfLWXsTQ+xsizXKTlhb4H3ea4uk0nKnSwgV1UVgN/GjpBeRyNlSo96UkRzjsioT6fKh/nqycr2kZoDWm6bvWGp1IoN7pMxzZLkvFvHHpw9Xu7dVEKm9rpditp57D9g5bdeGC6f9rtbGtnl07uVv/d3fynY/ps5VaxYUeLi4mTLli1Sv3597Yub9OnTS5YsWWTJkiWqgFDoU6ZMsfcGHiNAUW3atElWrFihxsrMmTPl4MGDuo8y2bNnVyV64MABTRgqCAenDhT3yZMnNWFANWzYUNtKfShKjJeNGzdqvlOnr1SjRg2JiopSRZ0rVy6fZWrVqiU5c+aUL774Qvbu3av9nT17thoSGEwYTxhVjMeiRYt03BiPpUuXeuqoWrWqhIWFyYQJE3Q/7cbowRBh/9y5c2XMmDG6D+Np9+7dWh/jjBGDgYgRxlh5k9y+u/HeZ5JJJqXRJBfkUM39srdyhNx1oojcNSajJPY9Iet22vunR8mGS7mS9pFWhUnmsbEy76N4Tx15exa6vM+q0cr07EtK8bLu9liJ72+fw06Ne2Z1lTkpS3NdWaZm//OyPVeU7LyirsCpfhXLMYvZJ7Oj7bzoHfJzzCVpVaWSp8yB2RNkepXOsqJvZ1n5ehdZ+VBh2bpoiry2/nI9TlJ5affJwcl3cLbTOxueA/18x2tMacIi/zMTfXDagQLFUy5WrJiULFnyijJ8osAee+wxiY2N1YgCHm21atV0DFBqlPHuk7uflG3WrJmMHDlSnn/+efWiiUQ4ZR988EHJnTu3vPnmm/Lee+9JmTJlVCk7ddesWVMVaJ8+fVS5VqpUSaMRKF0iGD/88IO0b99e873P7U70DYVcvXp1n/tJKO7u3btLwYIF1TOnv+PGjZOiRYtqPyiTOXNmadeunUYU6M+IESOkQIECnvHC4MDDx1AgavDyyy9rfxo3bqxeWOfOndXwIQJBVAajgfFlnMPDw9XgYIyJiFCfUy8pJX13H2+SSSal8XTRkhOWwZDpzVzSsHvmpLwWmSSnnJPTm+0yLfJe3kcqGSa5Iq3jLtnb7mTXx+dV+6zyCWNOSZR3vp2iPoqVmMhMUsl1rojuWay2JEr0tKvL+02RxaXlpViZvi5K5d/+dXtla+6K0rXyZZ1bqEkb6e/avli5hLS6dNFS6klREuQkKem78593ftKnA9vpLM/Nk+sU4ATgPsDXwYEItt8X12NKIq1CuPTbb7+VHj16eKZOQoGoBNMEgJJ2T1UYDGkd5BpCG2N4586dUrZsWXtP6mKmJFKJ3cdkwa1nJOu3ReTWlnbeVcTK8nxx4p5KgLB+EdK4R1Z7y0brOysRywtLtdJ2nocEWd/ouBzebG9GhknNhXklaTJW5Oinh2R1P996MXfA9l3N4vFfy/NHK8kPz5aRuUOmy4zI1jLqTpesPrJUHv98s+iMhEuftnyoswyoenkbXXtwzkS5f3NxmdrzdinqKntwzgS5dxFTEvdIUkzXKo/B4MtAIAw8atQoT6g4FAPAVxm8zY4dO0rt2rXtHP8Yg+HaYQzx9r///nud/8fDT+54cs137dollStXNtfCYHBhDIa/GCj4upbBMNqfQk6QDY0tJV8thzT/zFmzkJQX84hlMHT3YTDUPSd5lhWSqlcZDG7seiVMaixIMhqOfnZY1ozL5Nm+JjbOlHo/iAx8OpcMHbZfWj39sHSw7IUkeb1FXn97qeypf498YxkRnrwBv4tYBsNbtsHgyPZDc7+X+zcVk5961FODgXySrm34Lad88oZrDYO3scDniRMndGEb4WMnpEG+vxSoDHUQSj5+/PiVYRWTrktioeOAAQMkOjpaWrdurdfFV7lAiWkIwv4pOdYkk0wy6aZJ6CZh+tLHPiddunL6IfqTGDm42c8xdn0+pySuSJmlQDtLN16yytt5Ea0ySMbN8bLhkzivsilIlYrJXZcOyJDx+2Rb7iLSOE9SPg7j+fNMzVr62Jbf5C0au1hm0Bavdjt629L8Hv1PIr9IwXDre6zsik7KZxFkujNnzljfLxemEubQ3333XTUcnH3+CLYfWHTH/HaePHnsHN+YCIPBYLiZcWQkEQaicKxBuh5Yctn+Zrgmdh+ThQ3OSNaRRaROCzvPm5nRMrvzeXvDom0OKSNxsj8ytzTqxhsOlpJvekKitiTtdpO5r1MmVlYUiZOTSdk2GaXMwQJyRSDCbs9ZezMJH+VCYMnEUfLSDpEK9VrJ8CZXTh0fnDdZHvk9zt4SKV//NmmxeZlsa9xJ+lWxMo4uky5fbpVtvD5BRIFC6N10OeW5rm3l8QJJUYbfxo2QnluTdklEZUmXmJhoPQNJFgWfrEpn1fyqVas0z2AwGAwGw98P3nBjoT3LBngZACOBN9icaQl3gnQJCQm6hoG0detW6dWrl7FuDQaDwWBII2A4vP/++/p7OBgMTgK30aBrGIgskHgV0BgLBoPBYDCkHdD7o0eP9tgCThABnDzQH25iTo71AytXrtRMg8FgMBgMaYfVq1d7FkO6DQcHvqdnJ1/45Kd8DQaDwWAwpC3Q/46x4NgF4BgPoJMU7OSndw0Gg8FgMKRNsAOwBxzcxgJ4IgxMSRgMBoPBYEibYAdgDzjJMRY8EQa+ODsNBoPBYDCkTdyGgmMTuI0GfUuCKIM7DGEwGAwGgyFt4dgCjpHgfHe2NcLgzjAYDAaDwZD2cGwBJ8rgtgv47jEYnPCDwWAwGAyGtIc/Q8HZ1ikJJxkMBoPBYEibuO0Bt9HgkPTbjxa+dhoMBoPBYEgb+IssAN89UxIGg8FgMBjSNr4MBWdbIwzeBQwGg8FgMKQtXp132RbQz91DpEH+/JI3b16JiIi4PCVhMBgMBoMh7fLzc6/JfPu7yGx5tovImCNH5NixY3L8+BozJWEwGAwGgyGJyzZBMxk8r5sU99gHJU2EITVp166dpr8Db731lnz66af21rXTs2dPGTFihNxyyy12TvKoXbu2DB48WH788cdUbVdq0qpVK3niiScka9asds6No2PHjjJu3Dj9BK7ftYx3SnE/A3feeae2iWsfCsHuOe8+pmUKV2uq6a9Mlpz5pHTD9vp5vWnYc7imm5Fu3brJ+++/L1WqVLFz/PPggw/KJ598Ik2b3phrP6tHISlQoEDqT0kgJD/44AP55ptvVLi76du3r4wZM0buvvtuO+fvScOGDaV58+ZSvnx5O8eQWqCMc+fOrUrwo48+UkXId5TMzcJtt90md911l1SvXt3OSXvcrM+AY7z89NNPnsQ2sql48eJ2qSSKFCmixumXX34pFStWtHOTcO47jr3nnnvs3Msg+5CB7OecyaVsk8fl7gHzpVDVJnbOZep0elea95kmOQqWthTtI1Lhrn9Kpmw57b03P+WbPSmt+s+RXEUr6XZEqepSseUzUugvbvhcC3fccYfKjdKlS8vTTz8d0GjAWHj44Yf1/kQeFixY0N5zfdg75E55bGIb+S462p6SOJ56BkNCQoLMmzdPwsLCpFmzZnauSIsWLSQyMlI2btwoP//8s5379+Rf//qXPPPMM7J9+3Y7x5BaZMmSRWJiYuSHH364acf3zTfflMcee0x+//13OyftcbM/A7/++qvcf//98uKLL8rChQvVACAq5KZevXqSL18+yZUrl373BQ6St2MEVatWlRw5cthbyefwhnlyLvGUFLnlsgyFbHmKSK5ileTE3o0SF7VbFg95RuYP7CDn4mPtEn89Dq2bLTP63il7fhtv56Q95s+frzItPj5eihUr5tdocIyFbNmyyf79++Xrr7+WqKgoe+/14aqFCrs+k4z211RZxzBlyhSpW7euPjQ8aAjORo0a6V/AmjVrlpYhtIFAqVmzpmTOnFlOnjwpU6dOlfHjk24aPEZCHz169NBtrPSuXbvKokWLNBTD/kKFCum+8PBw+fzzz2Xu3Lm67dC+fXu577779MHlz3WuXbtWPQasJKA+LLvs2bNLXFycTJ8+XUaPHq37EAJPPvmkFC1aVLcPHjyoF2fp0qW63b17d2ncuLEKDIwkhM5XX32l351wqtN2Iipt2rTRkA5jsHXrVhkyZIjs27dP+/XUU09pHgYVbUEhfvvtt56x8uaBBx7Q+vC0qW/Dhg3qBVEf5+bHNhg79g0aNEitVm60PHny6L5t27ZpOcpDsHHKlCmTvPfee+ot8pviq1ev1igSfaX/KAfGi+tI27mGjlGIl0nomHOfPn066M1N3/y1lc8SJUpoOTzDAwcO6JjSPu4nPD76u27dOj3vI4884hnzNWvWePrka4w4xsG5JvSB6wDcbzzITtlAY+ZdlvMxVlxb7if+3jxG9WeffaZ1e4/RoUOHtJyvexoCjVFy+ec//6n9dfrBOHFtW7durYKJZ9IZg5deekmqVaumkZ1oy9vgWIQa48/zw3M/duxYLUt7wHkGvPF3XsYJcDgYT8aBa+S+57wJdg8GAoOGVLZsWY0w4LXxrAN943rSvsqVK2ueN5yL54Jzr1y5UvNoT40aNVT4p5TTR/dZRsEmyV2iimTJVUAST0Zrfr5ydTSacGj9HN12wuuLPnlSPyNKVJVb2vWR8EJl5NKF87Jv5TTZMOldqfvUh5IhcxZZ/Nn/s2T8RanW5mUpWLmRLBnaTc9Vsl4bqdSqm6wc/aoc3bFC6wKiAHWfHCgnD2yR/BXqqXKP2vybVL63lywb/oLmg7sdtTq8pdGPxJNHpECl+nLp4gXZt3yKrJ/0jjTo8aVElEyaHmv8/GjZ8vNgiY85aLXn39b+d+XA6hla14VzZyRrRGHJnse6/mfiZfucEZLN2i5+632SLkNGObFvk6wa/ZoeC2Xu6Cjl7+wsmXNEyJlTMbJp6oeyf+V03ReIvGVqSdWH/u0Zr6hNC2XNuDf1nFCkRgup+kBvCbPqjT9+SK9DWHgey1DrKucT46RmhwFWH2+XdOkzSPyx/bJ2/AA5tmuVHptcMBiA584xGoYNG6ZONngbC+5915MSXftIm7efkA4FJtk5bVN/DQMKNGPGjKpUCdlVqFBBhafjdSGQseoRMi+88ILs3btXBwMhHCr58+fXgUOwugU+YKjce++9snv3bq0fY4Dz/eMf/9D9jz/+uBoLeBrPP/+8bNq0SRU7kRDo0KGDPvgIKRKeLUqCPC4cBtDy5cvVcOCTfiJkvaEdjz76qMTGxkqfPn1k1KhRUqpUKT2OugCBW6ZMGTVWEMbp0qXzWRegXOgDygFhTMiTsUU5OiD0ELBffPGFKkXmklGulB8+fLgKRqd8sHECQl6EoV577TU1mNjvTCsx74YSQanQvyNHjuixlCGMy3kQ8u+8845OR2Hk+IO2BGoreShm7hW8Q86NN49QJ79Lly56HyC8UcAYJ/Tp+++/V2MMr9/BPUbJ9YJDGTNvMNp4uF999VXZsWOH1K9fXxo0aOAZI5QihjBjhCHAPeCLYGOUHLiPuZ8YB4xjnkWuJf1gPDECKlVKChszXvSB86IYGUsUJfcsxzIW3BOMfTACndfBebYZryVLlqgC9je+ge7BUMGIS58+vRp1QD/KlSuncoH+Fi5cWOWFN3v27FE5555+whDimbnW6ApGQebsuVWpORSs0kSV1pFtV0eviD7U7NBfTkXtkl9eb6aKr7BVvnyzLqrEslqGR7a8RSVDpiyW0q5m1R0huYsnebE5i5SXM3ExcvLgNt12Q/nwQuVk9ZjXZdO0T+zcwOQsXF4V7K9v3SO7fxsvRWq2lAKRDdSgwEggIrJg0GOyfbbv9QQcv23mMJn1/++X43vXqzFDG+cP6igrRvbWvjJtA8Xr3Cvlm3aRLTOGaKSCcYts3UvylAp8/XPkLyk12vezDIyjMstq5/KvX5I8ZWpKzUf7W89fem1DlftfkNhD22XOuw/L1l+GSo4CpeyjRSrd3V0NtOVWezgeQ6Xaw6/oNUspGA3IK+9Iw402Flatelsu3+1NZfDhw+okIGuPH/8i9Q2GmTNn6sPmKBcGgDwgz5meYJ5v586dqkhPnTqlkYlQQSEg8PHC6IgblDEC4I8//tD6Z8yYIb/88osewz7mi1A8CFz20za8Z4Qg4OFQ54oVK9T4wdOivShghIrj2aG4EXiElFCq3tx6662SIUMG9YjXr1+vNwT14SmjNIDzkofimTNnjtZJGNSXV0NEhvLUR7nxlie1ePFiNToc6M/AgQM1f9euXWpQkZy24hU5nnqgcXLg+8iRI/V6YhzRdwQo1xFBjXdIv+gf44CXR9vr1KmjHiR9oi2cm7L+QBkFamuoMK7OmNOn7777TuvDi3Rwj5EvrzUQoYyZN5QjosAYEhGifSh6xihnzpwacSCiRJ9XrfLvoaTWGAH3M20iMsO9jgLnOcV4RdmhDLnOGDU8r7Rzy5YkjxJDm/HjunPs5s2b9Zlxnp9ABDqvA14+ERrGi+tHOYwGb4Ldg8HgWhJp5NxcT0fJEx3FaHOcnDNnzuiz7A2GHtedNlAXYGzQH67PtYBRkBh71LOoMXu+4pbHX0GO7VwpZ0+f0Dw3RWvdLekzZJKtluI8l3DK8tZ/kZg/1ku+8rfK8T1rLc88k3V8RfWmM2bJIQmWQs9TunqScixSQWItY8HX1MbFC+dkx5yRcnDtLE+kIxgo2c1TP7aU6DE5sGqGRhnCC5a29wbnyPZlGiGgjXzi8e9ZPFGnYQ5vXKDePH2h7SXqPSQxe9bJH79P0n7vWvCtXLLanKf01feLm8LVm0v6jJllk9VOxjl682+yc94oibAMDaZ9ClZpbI1nRsvA+UxOH9mr7TiydYl9tEjmbDnV2IjZtVqP3/jTQNm/Yqq9N+V4Gw29evX6UyILgbgub0kgAFEuCEYeOicKQPgY4YLwcyAkj8GAQg4VQoVO+NAb6uOBJbqBcCU6gHc8adIkFWoofYQgSoWEZ4L3iyIABA8eFQYJXizeFmF/zse+EydO6JwnoVc8CuqYPXu2HuuGkDjHur0NR7E4UyqMEQLJDcIKz8UbhDb10QYHPNP//ve/9laSt+TAGNCG/v3765sFtBPl4vQz0Dg5eI8zbeN4riM3MdEWZxyZZiIag0J0QuaE2B3Y9kewtoYKdXAtuaZOu7jW7nrcY5RcQhkzb7zPxxg6YwT+7mNvUmuMAOHDuHz88cda14ABA/SaOnVhBKAE8XCIYnEfYOwAzypRnokTJ2obiHBgtNKnYAQ7L3CfOIYcY0PUwG0UOwS7B/3BolTKEqFj4Rh9ZcoR6DPGCfIJ44PrjXOBUUGkxQ1yDGOFiAJyAAPGMT68n+nkglGAcYDywqMuGNlIvX0Uty/CLe+XMH7TVybLfe+v0FTIUnqEy4kcnIs/KbmLV7Y87xpywVLAeOJ4yEwfhOXII8csxecLlL0Tog+Vi+fPyoVzifaWdb/zL/3V8swv1jkdLqnMuGR/XklYznw6XUA/nT7Tf8Yh2PkYLxQ+xoBDXPQebWv2fCV0/7mEWDUGfHFo/VzJkjO/NHv1R7m1y0CNLOycN9qnMZdc3EYD62huJmMBki9tQgAjgQcH5erM/QciFGETKgiZl19+WdcK8LDXqlVL/vOf/3he9QLahNBzJ5QvoAgwFCiDgEYIoYAAw4fFUkRH6BuhyrffflsFRijg/aREwKcEpliwThmP3r17ax8ZD4dQxikQRDumTZt2xRhyPmfeOzkEa2tywCjjGrnbRQg/NbjWMbsWUnOMWAeD4sTDZ2y4v/HkHTBKEVhEZjAYDh8+7JmnZ0qCkD1TKBgLKF687VAIdt7kkpJ70Fn0yDOMYsc4wBgDIlREVjCUHCOE6AJywIkKukHOMU5EFogAYkQwThitvoz+5MCagYyZs+raBUL6zNmz4NEfpw7vlBmvN5Upvet4EtMARA6IIDAVkdeqK2bPWjlqefEZs4brWgbLgpWY3f6jfzc7exZPuKLPJH/THYHIkCnM0oahyWYMrtn/fUC2zfpSMmXJLnWeeM8yHD5Qo+7vznXVXiykdHuWCAceUmdBIRD2xIpyh/VRqk6YL7nwhgbrBDBYUOYsRMMzxwM4evSoei+c3+0xON9pC8ciIDAgmCPFi0BwcjwCjzlywvMIu6FDh6pgIIzpDfM+hOXdoVqiKIwHAji5sBaC+giBOqCoWJPhC+aCORfRHqIcjKfbWAk0TsGgLSgJ1mQ4UD9eHxAq51wIX4dAhlKwtoYKC2iJxLjHnDYl917yt47gWsbMGzxnFJ4zZsFIrTECohu0m+k4lCX1uPtM/UxLENqnrDtKxj3MNAteEM+Sv7HyRbDzAnnO9eK5pN9EOLwJdg8GA0OD9ResDXGuH0qfPvFsOwYI0xbILSIj3hCBIHG/MdWJsco0qa9+JZfjf2zQtQDFarfWkP7Btb9e4bm7STgRJVly5de5dwemKAjbAxEEPGdC+TG712rd5xNOSdGarXQBn7OAMCSsfhGu/7PBEDqXEKdrMdyKOhSlfSp6j4SF55Ps+S9P5xGZIKJy+uhenVbJkDmbzzUJGBalGvxD8patLbsXjdO3VVjjoBGbApfvxZTiXrOAvnKmJ4K9cnmjSJnESSF46IQAEUSEc1HEKGAU4bJly7QMihYhwaCxrqFly5YaZgwVBBoLEVHuCA8GmcFn4PHOWNRFGJFpBaYGaAdh/bZt2+rxhCQ5N8ID5UwZwsoIEtqFp4cgoW48LcKlTgjVDUYFCoF1HNTDMSx2wytk/jy54JURiaEepnpY3EUf/b2Ly6p72sZ78YwjCzzdCjzQOAUDL4p1HPQfgcoYsYiNNyqItrD+A8XAj4vgmfEQBFoUF6ytvkCJMe4YCAh8+kBUCCXCwkSuH8qAED4eeSigGFBORA44nnuTPjpcy5h5w/Wk34xRkyZNdNEv5/VHSsbIH4wbzxx9oT4iBfTNDYYyfUPxOdMRgMFfsmRJj0Kl/aF606GcFyOBviEjWIDMOPta/xLsHgwGbWGNDVEBpiZQ+jgMhH+dKVRAZrBYlf2+7mHCxNTB+WmnL1nA/YihyXMbKhgHhzfO14WPhNiPbPMfqWXNwoVzZyWydU9VfCzqq//sMF2IB7zRwBsGhPdP7NuodR/fu8EyMMpZxsN6v4aINyhTDJES9dpYCteSf007X2GkBCPhxGEmGFSxMhVyLdBm3qwIL1RW+82aBN5saPLSeJ+/YeHm0NpZOnVSscXT+qNR+SvcJmUaPSrH96yTk/u3SPSWJcyH6H6MimK175H8FW/XYzmOaZAqD7yoUZuMYdnUaGG9x4WzyVsT5Y33Akem7nwthPwzSY+VjTeB13Qj4BVEFjShoFk8hYfAfCaLuIA5HNqD98xKaV6dTM68M3OrhB3x+lnYiNCnPue1SUKReAEYBswLcYEwIvA48BYIZyIk33jjDZ1jZRphwoQJqqA4ljAkAomFWwgaBJevECjlCNui0KkHAwWviuN8CZVg4FmOHz9ehQ7rJxgfQsfU5wvGccGCBapMCZsz3034FaWDUA42TsEgLI9gZdU744gxhCHE2DKOhKrx+AjhY9wECj0Ha6svGEOuG9Ei7hP6wQJS2s95+/XrJ6+88ookJiZq/aFAf1jEiCLmeNYq4Mk6XOuYuXGPEQqSN2qY5vJHSsbIH9yvjB33McYUBjneDPe6AwYN7WEdimPMAwsRMQZ524noFtEyjCyUZjBCOS+RF4QjRvztt9+u5+a+90WgezAUnAXayAIcB4xPjANvKMM4+zLoOBdGBVE1nnlf4DDQNnc0JBR41e9M3HFV7kwr+AOPmFcNWYzX7LUfVWmeP3Na3zaAU4d36SLCuOg/9FVKINLAPD2focIrjTvnfSNFa7aQu17/WUrUvV8ST4U+pcRrm/HHDujrlyj5a2X3wrGy/dcvVaG3/t9iqd72P2pERG1cYJfwTdyRP2TN2H66CLR53+lS98lBGnVZO2GAvnrKeG6c8qG+Btr0399LxZZd1VgCouYcG2+NI0YZP7IVUaKKbPjxfa03pfh7G4Ln/kYaDTzPPPPc08gX2uJ8ktJZG5fwGhDAnTp1sg8zGAzXG7xnBIFjQPJ7B3jW3r8PYTAYbixMbbgjLyxuDMuR2/O7F6lJKK9OhlImNcApxonBAcBIJhHZJulUmzEYDIYbD4YBPzpEZG/UqFEa7iYSw/oAfq7YkDap0+kdKez1K4+GGweLJnmjpFbHt2Xv0smya+FY/b2Hyvf+S1/brNDiabvk1ZzYv1kWfuh7TZk/WDjP78gQSQ9mCLiNBuQEvxMU6JXulBCywUDo1t8COoPBkPp4/3Ij015M2TFdYTAY/jxK1W8n5Zs/JVnC82qkYf/qGfrLmaxVSG1YXM96PqaXg0UNkBcsvp48ebKuwUltcF6YKjQGg8FgMBgMBr+4DQbWFrGg2W0w3NC3JAwGg8FgMPw1MQaDwWAwGAyGoBiDwWAwGAwGQ1CMwWAwGAwGg0EJ9CulajBc68+YGgwGg8Fg+HtjIgwGg8FgMBh8QkDBScZgMBgMBoPB4MFtJLhJ7yvTYDAYDAZD2iOQPaAGAxijwWAwGAyGtEswe8BjMBgMBoPBYDD4NRj4zxgNBoPBYDAY3FEGdwLPokd+J9pgMBgMBkPaJJAdgNHgWfRI4i9TGQwGg8FgSFug/932gDs5XGEwVK1a1c42GAwGg8GQVkD/u+0BEjifIiL/BwOy1M+TDKudAAAAAElFTkSuQmCC)"
      ],
      "metadata": {
        "id": "cWl3br7Dmi7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "DRXM-Wze75sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Necessary Libraries\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "print(\"\\nLoading dataset...\")\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/INFO 5731/Week 13/Amazon_Unlocked_Mobile.csv')\n",
        "\n",
        "# Ensure the 'Reviews' column exists and has no missing values\n",
        "if 'Reviews' not in df.columns:\n",
        "    raise ValueError(\"'Reviews' column not found in the dataset!\")\n",
        "df = df.dropna(subset=['Reviews'])  # Drop rows with NaN in 'Reviews'\n",
        "\n",
        "# Clean the reviews\n",
        "print(\"\\nPreprocessing reviews...\")\n",
        "df['Cleaned_Reviews'] = df['Reviews'].str.lower()  # Convert to lowercase\n",
        "df['Cleaned_Reviews'] = df['Cleaned_Reviews'].apply(\n",
        "    lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))  # Remove special characters\n",
        "tokenized_reviews = df['Cleaned_Reviews'].apply(lambda x: x.split())  # Tokenize the reviews\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGK5FRs08GRi",
        "outputId": "6a0c5269-a6a5-480b-ac8d-4d2a1823351a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading dataset...\n",
            "\n",
            "Preprocessing reviews...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Train Word2Vec model\n",
        "print(\"\\nTraining Word2Vec model...\")\n",
        "word2vec_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=2, workers=4, sg=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYL8Wca0fYrL",
        "outputId": "5a29a2db-3a8b-4d91-ba1a-07d1d9773e23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Word2Vec model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Generate feature vectors for clustering\n",
        "print(\"\\nGenerating feature vectors...\")\n",
        "def average_word_vectors(words, model, vector_size):\n",
        "    \"\"\"Compute the average word vectors for a list of words.\"\"\"\n",
        "    feature_vec = np.zeros((vector_size,), dtype=\"float32\")\n",
        "    n_words = 0\n",
        "    for word in words:\n",
        "        if word in model.wv.key_to_index:\n",
        "            n_words += 1\n",
        "            feature_vec = np.add(feature_vec, model.wv[word])\n",
        "    if n_words > 0:\n",
        "        feature_vec = np.divide(feature_vec, n_words)\n",
        "    return feature_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he3Nm2ynfcmZ",
        "outputId": "739711ee-3339-40f4-8f20-9d7910818264"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating feature vectors...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to each review\n",
        "vector_size = 100\n",
        "review_vectors = np.array(\n",
        "    [average_word_vectors(review, word2vec_model, vector_size) for review in tokenized_reviews]\n",
        ")"
      ],
      "metadata": {
        "id": "c-G-HDyefilG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Apply K-Means clustering\n",
        "print(\"\\nClustering reviews with K-Means...\")\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans.fit(review_vectors)\n",
        "\n",
        "# Add cluster labels to the DataFrame\n",
        "df['Word2Vec_Cluster'] = kmeans.labels_\n",
        "\n",
        "# Display results\n",
        "print(\"\\nClustered Data Sample:\")\n",
        "print(df[['Reviews', 'Word2Vec_Cluster']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPrnX5tlfpuV",
        "outputId": "04d86763-b21a-4728-a8b0-96ceea6b4092"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Clustering reviews with K-Means...\n",
            "\n",
            "Clustered Data Sample:\n",
            "                                             Reviews  Word2Vec_Cluster\n",
            "0  I feel so LUCKY to have found this used (phone...                 3\n",
            "1  nice phone, nice up grade from my pantach revu...                 0\n",
            "2                                       Very pleased                 1\n",
            "3  It works good but it goes slow sometimes but i...                 0\n",
            "4  Great phone to replace my lost phone. The only...                 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT"
      ],
      "metadata": {
        "id": "rOfGo3Ci79xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the full dataset\n",
        "print(\"\\nLoading and preprocessing data...\")\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/INFO 5731/Week 13/Amazon_Unlocked_Mobile.csv')\n",
        "df = df.dropna(subset=['Reviews'])\n",
        "\n",
        "# Initialize BERT\n",
        "print(\"\\nInitializing BERT model...\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Generate BERT embeddings for the full dataset\n",
        "def generate_bert_embeddings(texts, tokenizer, model, max_len=128):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            # Tokenize the text\n",
        "            inputs = tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=max_len\n",
        "            )\n",
        "            input_ids = inputs[\"input_ids\"].to(device)\n",
        "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "            embeddings.append(cls_embedding.cpu().numpy())\n",
        "    return torch.tensor(embeddings).squeeze(1)\n",
        "\n",
        "print(\"\\nGenerating BERT embeddings for all reviews...\")\n",
        "texts = df['Reviews'].tolist()  # Use the full dataset\n",
        "bert_embeddings = generate_bert_embeddings(texts, tokenizer, model)\n",
        "print(f\"BERT Embeddings Shape: {bert_embeddings.shape}\")\n",
        "\n",
        "# Apply K-Means clustering\n",
        "print(\"\\nApplying K-Means clustering on BERT embeddings...\")\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans.fit(bert_embeddings)\n",
        "\n",
        "# Add the cluster labels to the full DataFrame\n",
        "df['BERT_KMeans_Cluster'] = kmeans.labels_\n",
        "\n",
        "# # PCA for visualization (commented out due to large dataset size)\n",
        "# print(\"\\nReducing dimensions with PCA for visualization...\")\n",
        "# pca = PCA(n_components=2, random_state=42)\n",
        "# pca_embeddings = pca.fit_transform(bert_embeddings)\n",
        "\n",
        "print(\"\\nPlotting the clusters...\")\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(pca_embeddings[:, 0], pca_embeddings[:, 1], c=df['BERT_KMeans_Cluster'], cmap='viridis', s=10)\n",
        "plt.title(\"BERT Clustering Visualization\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Display a sample of the clustered data\n",
        "print(\"\\nClustered Data Sample:\")\n",
        "print(df[['Reviews', 'BERT_KMeans_Cluster']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwMETbDFhlrZ",
        "outputId": "604d994e-6fd2-44ac-dd07-63dd2e930935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading and preprocessing data...\n",
            "\n",
            "Initializing BERT model...\n",
            "\n",
            "Generating BERT embeddings for all reviews...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm going to submit the exercise while the BERT Model is running. I've run it four times, but the Google Colab environment keeps crashing and it takes a very long time to run. I believe the code would work in a more appropriate environment."
      ],
      "metadata": {
        "id": "5Gzv3Zz4oKo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ],
      "metadata": {
        "id": "tRijW2aLGONl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write your response here:**\n",
        "\n",
        "I've struggled to get my code to run in a timely manner which is required to compare my results. Instead, I will compare what I can see from Ms. Fengjiao's example and what I have read regarding these methods.\n",
        "\n",
        "The clustering methods had different strengths and weaknesses. K-Means worked well and gave balanced clusters but required picking the number of clusters ahead of time. DBSCAN was good at finding outliers but needed careful parameter tuning. Hierarchical Clustering showed relationships between clusters nicely with a dendrogram but didn't scale well for large datasets. Word2Vec + K-Means grouped reviews based on word meanings, but some detail was lost when averaging word vectors. BERT created meaningful clusters but was slow and needed a lot of resources causing me to use a keyboard interrupt.\n",
        "\n",
        "Each method had its own use depending on the situation. As with anything in life, there's a tradeoff for each method so you should understand each and know when to use them."
      ],
      "metadata": {
        "id": "pIYCj5qyGfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Overall I think these assignments have become more computationaly intensive than I imagined. We don't have access to the appropriate hardware to run this kind of\n",
        "code. I spent two days running code and testing it just for question 1. My first BERT model would have taken 20+ hours to run.\n",
        "\n",
        "We don't have access to GPUs on here and its not fair to require work without access to the required hardware to complete the assignment. You should grade the code,\n",
        "not the output. The CPUs can't run the code.\n",
        "\n",
        "Question (1) should be it's own exercise. It took an incredibly long time to work. I chose to train the model for 1 epoch per fold during 10-fold cross-validation\n",
        "due to the computational cost of fine-tuning BERT. I chose to do this to balance time constraints while still allowing for an evaluation of model performance.\n",
        "I also decided to use a keyboard interrupt because of how long it was taking. I ran it once that took over ten hours and lost my internet connection. I started over\n",
        "but I didn't have enough time to run everything to a perfect standard.\n",
        "\n",
        "Question (2)\n",
        "Basically the same as question 1. My K-means ran for an hour before I stopped it the first time. Overall, it is taking too long to run the code on this hardware.\n",
        "While I gained a deeper understanding for the models I'm still focused on how long it is taking to run the code. Maybe I can gain remote access to the lab in\n",
        "the College of Information and try running some of these again.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}