{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeChastain84/Mike_INFO5731_Fall2024/blob/main/Chastain_Mike_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mike Chastain Assignment 2"
      ],
      "metadata": {
        "id": "dK-pHLUMJ8Mu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "\"\"\"\n",
        "A small residential cleaning company named Mil Maids was created with the goal of generating job\n",
        "opportunities for military spouses. They are located in Killeen, TX near Fort Cavazos, TX. They\n",
        "recently decided to expand the business to towns outside of other military installations, but they need\n",
        "help selecting the best location. Expansion is expensive so they want to be sure they're expanding to\n",
        "an area where the demand is high and there are plenty of military spouses to meet the demand.\n",
        "\n",
        "Research question: Where should Mil Maids expand to if they want to be sure that the demand is high and\n",
        "also maximizes job opportunities for military spouses?\n",
        "\n",
        "Data Analysis Requirements:\n",
        "This is a simple project that aims to collect a medium amount of data related to .\n",
        "\n",
        "Data considerations:\n",
        "- How many military personnel are stationed at different military installations.?\n",
        "    - The installations with the highest number of military personnel should have higher numbers of military spouses seeking employment.\n",
        "\n",
        "- How much residential cleaning demand is in each area?\n",
        "    - Using Beautiful Soup web scraping techniques to determine the demand for residential cleaning\n",
        "      in towns around different military installations.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e8187192-f0d4-4ebd-e7d2-5fa94872cd5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nA small residential cleaning company named Mil Maids was created with the goal of generating job \\nopportunities for military spouses. They are located in Killeen, TX near Fort Cavazos, TX. They \\nrecently decided to expand the business to towns outside of other military installations, but they need \\nhelp selecting the best location. Expansion is expensive so they want to be sure they're expanding to\\nan area where the demand is high and there are plenty of military spouses to meet the demand.\\n\\nResearch question: Where should Mil Maids expand to if they want to be sure that the demand is high and\\nalso maximizes job opportunities for military spouses?\\n\\nData Analysis Requirements:\\nThis is a simple project that aims to collect a medium amount of data related to .\\n\\nData considerations:\\n- How many military personnel are stationed at different military installations.?\\n    - The installations with the highest number of military personnel should have higher numbers of military spouses seeking employment.\\n\\n- How much residential cleaning demand is in each area?\\n    - Using Beautiful Soup web scraping techniques to determine the demand for residential cleaning \\n      in towns around different military installations.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "!pip3 install pytrends  -q            # I like to suppress \"Requirement already satisfied\" outputs.\n",
        "from pytrends.request import TrendReq # Imports TrendReq function\n",
        "import pandas as pd                   # Imports pandas module\n",
        "import time                           # Imports time function\n",
        "import random                         # Imports random function\n",
        "\n",
        "pytrend = TrendReq()                  # An instance of TrendReq (Google Trends API) to make requests of Google Trends.\n",
        "\n",
        "# Dictionary of the 6 military installations with the most service members and their locations.\n",
        "# Fort Cavazos, TX was stripped from the list because Mil Maids is located there:\n",
        "locations = {\n",
        "    \"Fort Liberty\": \"Fort Liberty, North Carolina\",\n",
        "    \"Joint Base San Antonio\": \"Joint Base San Antonio, Texas\",\n",
        "    \"Joint Base Lewis-McChord\": \"Joint Base Lewis-McChord, Washington\",\n",
        "    \"Naval Station Norfolk\": \"Naval Station Norfolk, Virginia\",\n",
        "    \"Camp Pendleton\": \"Camp Pendleton, California\",\n",
        "    \"Fort Campbell\": \"Fort Campbell, Kentucky\"\n",
        "}\n",
        "\n",
        "# Keywords to search for in Google Trends:\n",
        "keywords = [\"house cleaning\"]\n",
        "\n",
        "# Function to scrape cleaning service information using Google Trends:\n",
        "def scrape_cleaning_demand(locations, keywords):\n",
        "    trends_data = pd.DataFrame()  # An empty DataFrame to store the scraped data.\n",
        "\n",
        "    for location, full_name in locations.items():\n",
        "        state_name = full_name.split(',')[1].strip()\n",
        "        delay = random.uniform(1, 3)                    # generates a random integer between 1 and 3\n",
        "        time.sleep(delay)                               # uses the delay generated above to avoid appearing like a bot or overloading the server.\n",
        "\n",
        "        # The next line is tricky. I started geo fencing by city and then state codes, but it was being rejected by the google servers.\n",
        "        # I settled with 'US' for testing purposes and got it working. It worked, so I am now aligning installations with their state.\n",
        "        try:\n",
        "            # We first fetch data for the entire US and print it by state.\n",
        "            pytrend.build_payload(kw_list=keywords, geo='US', timeframe='today 12-m')\n",
        "            data = pytrend.interest_by_region(resolution='REGION', inc_low_vol=True, inc_geo_code=False)\n",
        "#           print(f'\\nData by state:\\n', data)\n",
        "\n",
        "            # Filter the data for the state and make a copy. Not making a copy generates a warning.\n",
        "            state_data = data[data.index == state_name].copy()\n",
        "            # print filtered data for troubleshooting:\n",
        "            print(f'\\nData for {location} ({state_name}):\\n', state_data)\n",
        "\n",
        "            # Add state data to the trends_data DataFrame\n",
        "            if not state_data.empty:\n",
        "                state_data.loc[:, 'Location'] = location\n",
        "                trends_data = pd.concat([trends_data, state_data])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred for {location}: {e}\")\n",
        "\n",
        "    return trends_data.reset_index()\n",
        "\n",
        "trends_result = scrape_cleaning_demand(locations, keywords)\n",
        "print(trends_result)\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61333e0e-7762-4529-f8fc-e3a6ccd15e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data for Fort Liberty (North Carolina):\n",
            "                 house cleaning\n",
            "geoName                       \n",
            "North Carolina              83\n",
            "\n",
            "Data for Joint Base San Antonio (Texas):\n",
            "          house cleaning\n",
            "geoName                \n",
            "Texas                80\n",
            "\n",
            "Data for Joint Base Lewis-McChord (Washington):\n",
            "             house cleaning\n",
            "geoName                   \n",
            "Washington              91\n",
            "\n",
            "Data for Naval Station Norfolk (Virginia):\n",
            "           house cleaning\n",
            "geoName                 \n",
            "Virginia              90\n",
            "\n",
            "Data for Camp Pendleton (California):\n",
            "             house cleaning\n",
            "geoName                   \n",
            "California              83\n",
            "\n",
            "Data for Fort Campbell (Kentucky):\n",
            "           house cleaning\n",
            "geoName                 \n",
            "Kentucky              76\n",
            "          geoName  house cleaning                  Location\n",
            "0  North Carolina              83              Fort Liberty\n",
            "1           Texas              80    Joint Base San Antonio\n",
            "2      Washington              91  Joint Base Lewis-McChord\n",
            "3        Virginia              90     Naval Station Norfolk\n",
            "4      California              83            Camp Pendleton\n",
            "5        Kentucky              76             Fort Campbell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "class=\"c-article-title\"\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "class=\"app-article-masthead__journal-title\"\n",
        "\n",
        "(3) Year\n",
        "data-test=\"article-publication-year\"\n",
        "Test this and change to entire class if doesn't work\n",
        "\n",
        "(4) Authors\n",
        "class=\"c-article-author-list__item\"\n",
        "\n",
        "(5) Abstract\n",
        "class=\"c-article-section\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The CSV output was uploaded to Mike_INFO5731_Fall2024 GITHUB repository\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Step1: Define the URL to search for articles about \"cat\"\n",
        "base_url = 'https://link.springer.com/search?new-search=true&query=cat'\n",
        "\n",
        "# Step 2: Add headers to simulate a browser request\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "data = [] # an empty list to append data to later.\n",
        "\n",
        "for page_number in range(1, 51):   # pages 1 through 50\n",
        "    # construct the url\n",
        "    url = base_url + str(page_number)\n",
        "\n",
        "    # Step 3: Request the page\n",
        "    page = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    # Step 4: Find all entries containing articles\n",
        "    entries = soup.find_all('h3', class_='app-card-open__heading')\n",
        "\n",
        "    # Step 5: Extract title and URL for each article\n",
        "    for entry in entries:\n",
        "        title_tag = entry.find('a')  # Find the <a> tag inside the <h3>\n",
        "        if title_tag:\n",
        "            title = title_tag.text.strip()  # Get the text inside the <a> tag, which is the title\n",
        "            url = \"https://link.springer.com\" + title_tag['href']  # Get the href attribute and prepend the base URL\n",
        "            data.append({'Title': title, 'URL': url})\n",
        "            print(f'Title: {title}, URL: {url}')\n",
        "\n",
        "        # Random delay between requests to avoid triggering anti-bot mechanisms\n",
        "        delay = random.uniform(1, 4)\n",
        "        time.sleep(delay)\n",
        "\n",
        "# Step 6: Save the results to a CSV file\n",
        "df = pd.DataFrame(data)\n",
        "csv_filename = 'paper_titles_and_urls.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "# For Google Colab: Download the CSV file\n",
        "from google.colab import files\n",
        "files.download(csv_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NnPCRNHeTUU",
        "outputId": "40b3869f-c5cb-4f1b-d9bb-f8e16073a2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Physiological roles of catalases Cat1 and Cat2 in Myxococcus xanthus, URL: https://link.springer.com/article/10.1007/s12275-022-2277-7\n",
            "Title: Screening of commonly prescribed drugs for effects on the CAT1-mediated transport of l-arginine and arginine derivatives, URL: https://link.springer.com/article/10.1007/s00726-022-03156-2\n",
            "Title: Erratum to: Physiological roles of catalases Cat1 and Cat2 in Myxococcus xanthus, URL: https://link.springer.com/article/10.1007/s12275-023-00025-9\n",
            "Title: Overexpression of bovine leukemia virus receptor SLC7A1/CAT1 enhances cellular susceptibility to BLV infection on luminescence syncytium induction assay (LuSIA), URL: https://link.springer.com/article/10.1186/s12985-020-01324-y\n",
            "Title: Homogeneous and Heterogeneous α-Diimine Ni(II) Catalysts with Asymmetric Fluorine-Containing Diphenylmethyl Substituents for the Polymerization of Ethylene, URL: https://link.springer.com/article/10.1007/s10562-023-04444-6\n",
            "Title: Investigating novel antifungal strategies through molecular docking & dynamics simulations of oxidative stress response in Candida albicans, URL: https://link.springer.com/article/10.1007/s13721-024-00464-3\n",
            "Title: The catE gene of Bacillus licheniformis M2-7 is essential for growth in benzopyrene, and its expression is regulated by the Csr system, URL: https://link.springer.com/article/10.1007/s11274-023-03630-3\n",
            "Title: Structure–Performance Relationship(SPR) of Ziegler Natta catalysts(TiCl4/MgCl2-based) in ethylene/1-butene and ethylene/1-hexene copolymerization, URL: https://link.springer.com/article/10.1007/s10965-022-03173-5\n",
            "Title: Regulation Transcriptional of Antibiotic Resistance Genes (ARGs) in Bacteria Isolated from WWTP, URL: https://link.springer.com/article/10.1007/s00284-023-03449-z\n",
            "Title: Induction of disease symptoms of a non-cognate helper begomovirus by CLCuMB βC1 through the salicylic acid-dependent pathway, URL: https://link.springer.com/article/10.1007/s13562-022-00814-0\n",
            "Title: Exogenous Epigallocatechin-3-Gallate Alleviates Pesticide Phytotoxicity and Reduces Pesticide Residues by Stimulating Antioxidant Defense and Detoxification Pathways in Melon, URL: https://link.springer.com/article/10.1007/s00344-023-11092-y\n",
            "Title: Comparison of clinical characteristics between chronic bronchitis and non-chronic bronchitis in patients with chronic obstructive pulmonary disease, URL: https://link.springer.com/article/10.1186/s12890-022-01854-x\n",
            "Title: Ferrichrome, a fungal-type siderophore, confers high ammonium tolerance to fission yeast, URL: https://link.springer.com/article/10.1038/s41598-022-22108-0\n",
            "Title: General control nonderepressible 1 interacts with cationic amino acid transporter 1 and affects Aedes aegypti fecundity, URL: https://link.springer.com/article/10.1186/s13071-022-05461-x\n",
            "Title: Sequencing and gene expression analysis of catalase genes in Antarctic fungal strain Penicillium griseofulvum P29, URL: https://link.springer.com/article/10.1007/s00300-021-03001-4\n",
            "Title: Iron Oxide Nanoparticle-Induced Changes in the Growth, Oxidative Responses, and Antioxidant Capacity of Physalis alkekengi Seedlings, URL: https://link.springer.com/article/10.1134/S1021443722602944\n",
            "Title: Hydroxychloroquine an Antimalarial Drug, Exhibits Potent Antifungal Efficacy Against Candida albicans Through Multitargeting, URL: https://link.springer.com/article/10.1007/s12275-024-00111-6\n",
            "Title: Effects of the kinetic pattern of dietary glucose release on nitrogen utilization, the portal amino acid profile, and nutrient transporter expression in intestinal enterocytes in piglets, URL: https://link.springer.com/article/10.1186/s40104-024-01000-z\n",
            "Title: Bacillus Consortia Modulate Transcriptional and Metabolic Machinery of Arabidopsis Plants for Salt Tolerance, URL: https://link.springer.com/article/10.1007/s00284-023-03187-2\n",
            "Title: Dietary lysine level affects digestive enzyme, amino acid transport and hepatic intermediary metabolism in turbot (Scophthalmus maximus), URL: https://link.springer.com/article/10.1007/s10695-022-01098-w\n",
            "Title: Semi-implicit-Type Order-Adaptive CAT2 Schemes for Systems of Balance Laws with Relaxed Source Term, URL: https://link.springer.com/article/10.1007/s42967-024-00414-w\n",
            "Title: Physiological roles of catalases Cat1 and Cat2 in Myxococcus xanthus, URL: https://link.springer.com/article/10.1007/s12275-022-2277-7\n",
            "Title: Homogeneous and Heterogeneous α-Diimine Ni(II) Catalysts with Asymmetric Fluorine-Containing Diphenylmethyl Substituents for the Polymerization of Ethylene, URL: https://link.springer.com/article/10.1007/s10562-023-04444-6\n",
            "Title: Erratum to: Physiological roles of catalases Cat1 and Cat2 in Myxococcus xanthus, URL: https://link.springer.com/article/10.1007/s12275-023-00025-9\n",
            "Title: Spatially patterned hydrogen peroxide orchestrates stomatal development in Arabidopsis, URL: https://link.springer.com/article/10.1038/s41467-022-32770-7\n",
            "Title: Structure–Performance Relationship(SPR) of Ziegler Natta catalysts(TiCl4/MgCl2-based) in ethylene/1-butene and ethylene/1-hexene copolymerization, URL: https://link.springer.com/article/10.1007/s10965-022-03173-5\n",
            "Title: Exogenous serotonin improves drought and salt tolerance in tomato seedlings, URL: https://link.springer.com/article/10.1007/s10725-023-01016-x\n",
            "Title: Differential expression of genes potentially related to the callogenesis and in situ hybridization of SERK gene in macaw palm (Acrocomia aculeata Jacq.) Lodd. ex Mart., URL: https://link.springer.com/article/10.1007/s00709-023-01881-3\n",
            "Title: Biochemical probing of phloem sap defensive traits in Brassica juncea–B. fruticulosa introgression lines following Lipaphis erysimi infestation, URL: https://link.springer.com/article/10.1007/s12298-023-01341-5\n",
            "Title: Hydrogen sulfide attenuates intracellular oxidative stress via repressing glycolate oxidase activities in Arabidopsis thaliana, URL: https://link.springer.com/article/10.1186/s12870-022-03490-3\n",
            "Title: Using the Superfolder GFP (sfGFP) System to Study Plant Peroxisomal Protein Import, URL: https://link.springer.comhttps://link.springer.com/chapter/10.1007/978-1-0716-3048-8_31\n",
            "Title: Early Leishmania infectivity depends on miR-372/373/520d family-mediated reprogramming of polyamines metabolism in THP-1-derived macrophages, URL: https://link.springer.com/article/10.1038/s41598-024-51511-y\n",
            "Title: Comparison of clinical characteristics between chronic bronchitis and non-chronic bronchitis in patients with chronic obstructive pulmonary disease, URL: https://link.springer.com/article/10.1186/s12890-022-01854-x\n",
            "Title: Identification of Germplasm and Sugar Transporter Gene ZmSWEET1b Associated with Salt Tolerance in Maize, URL: https://link.springer.com/article/10.1007/s00344-023-11033-9\n",
            "Title: Amino acid profiles, amino acid sensors and transporters expression and intestinal microbiota are differentially altered in goats infected with Haemonchus contortus, URL: https://link.springer.com/article/10.1007/s00726-023-03235-y\n",
            "Title: A natriuretic peptide from Arabidopsis thaliana (AtPNP-A) can modulate catalase 2 activity, URL: https://link.springer.com/article/10.1038/s41598-020-76676-0\n",
            "Title: Heat, drought, and combined stress effect on transgenic potato plants overexpressing the StERF94 transcription factor, URL: https://link.springer.com/article/10.1007/s10265-023-01454-8\n",
            "Title: Development of semi-synthetic catalyst based on clay and their use in catalytic cracking of petroleum residue, URL: https://link.springer.com/article/10.1007/s13203-021-00268-w\n",
            "Title: Molecular characterization of carbapenem resistant E. coli of fish origin reveals the dissemination of NDM-5 in freshwater aquaculture environment by the high risk clone ST167 and ST361, URL: https://link.springer.com/article/10.1007/s11356-023-25639-9\n",
            "Title: Access block and overcrowding at the emergency department at Tupua Tamasese Meaole Hospital in Samoa, URL: https://link.springer.com/article/10.1186/s12245-023-00512-1\n",
            "Title: Extracellular arginine is required but the arginine transporter CAT3 (Slc7a3) is dispensable for mouse normal and malignant hematopoiesis, URL: https://link.springer.com/article/10.1038/s41598-022-24554-2\n",
            "Title: Role of Silicon in Mediating Salt Stress Responses in Arabidopsis Methylation Mutants, URL: https://link.springer.com/article/10.1007/s42729-024-01848-0\n",
            "Title: The Effect of Post-activation Performance Enhancement of Coincidence Anticipation Timing in Basketball Players, URL: https://link.springer.com/article/10.1007/s41465-023-00277-8\n",
            "Title: Bradyrhizobium japonicum IRAT FA3 promotes salt tolerance through jasmonic acid priming in Arabidopsis thaliana, URL: https://link.springer.com/article/10.1186/s12870-022-03977-z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one\n",
        "\n",
        "**reddit_data.csv is saved to Github**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "I57NXsauCec2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "768f68c2-ab38-4a8b-9f26-cabd7c7f8668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected Reddit data and saved it to \"reddit_data.csv\"\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "# I worked 4A many times and finally got it to work using what I believe is the 4B method.\n",
        "# I had to create my own app in Reddit and then use its attributes here to enable PRAW\n",
        "\n",
        "!pip install praw -q\n",
        "import pandas as pd                 # Import pandas for handling and analyzing data\n",
        "import praw                         # Import the Python Reddit API Wrapper (PRAW)\n",
        "\n",
        "# Function to fetch Reddit data using PRAW API\n",
        "def fetch_reddit_data_praw(keyword, num_posts=100):\n",
        "    #Initialize Reddit API client\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=\"Qozsx3NdZjOnISMox13WmQ\",\n",
        "        client_secret=\"Uuvba-j-yxAg1fERUB4XjOVl4STVdQ\",\n",
        "        user_agent=\"MyFirstApp/0.1 by DataSci_as_a_Service\"\n",
        "    )\n",
        "\n",
        "    posts_data = []                 # A simple list to hold the collected data\n",
        "\n",
        "    #Search Reddit using provided keyword:\n",
        "    for submission in reddit.subreddit(\"all\").search(keyword, limit=num_posts):\n",
        "        posts_data.append({\n",
        "            'Title': submission.title,              # Extract post title\n",
        "            'Subreddit': submission.subreddit.display_name,                    # Extract subreddit name\n",
        "            'Username': submission.author.name if submission.author else 'N/A', # Extract author's username or use 'N/A'\n",
        "            'Upvotes': submission.score,            # Extract upvote count\n",
        "            'Created_UTC': submission.created_utc,   # Extract timestamp of post creation\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(posts_data)  # Create a DataFrame from the list of posts\n",
        "    return df\n",
        "\n",
        "# Test the function by searching for the keyword 'python' and retrieving 100 posts\n",
        "keyword = 'python'\n",
        "reddit_df = fetch_reddit_data_praw(keyword, 100)\n",
        "\n",
        "# Check if reddit_df is not None before attempting to save it:\n",
        "if reddit_df is not None:\n",
        "    # Save the collected data to a CSV file\n",
        "    reddit_df.to_csv('reddit_data.csv', index=False)\n",
        "    print('Collected Reddit data and saved it to \"reddit_data.csv\"')  # Print a message when done\n",
        "else:\n",
        "    print( 'Failed to collect Reddit data. No data to save.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I spent most of Saturday and Sunday trying to understand what I was getting wrong. I went through many different approaches solving these.\n",
        "I think we should extend the due date on these assignments until the end of Monday or Tuesday in the future. I can't see the TA on Thursday\n",
        "or Friday because of my personal schedule and class schedule.\n",
        "I work on this stuff over the weekend and if we had Monday to discuss with them I could speak with them and my errors before submitting.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}