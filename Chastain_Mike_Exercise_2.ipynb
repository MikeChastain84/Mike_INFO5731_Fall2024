{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeChastain84/Mike_INFO5731_Fall2024/blob/main/Chastain_Mike_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mike Chastain Assignment 2"
      ],
      "metadata": {
        "id": "dK-pHLUMJ8Mu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "\"\"\"\n",
        "A small residential cleaning company named Uniform Cleaners was created with the goal of creating job opportunities for military spouses.\n",
        "They are located in Killeen, TX near Fort Cavazos, TX. They want to expand to towns outside of other military installations but need help\n",
        "selecting the best location.\n",
        "\n",
        "Research question: Where should Uniform cleaning expand to maximize job opportunities for military spouses and business profitability?\n",
        "\n",
        "Data Analysis Requirements:\n",
        "This is a simple project that aims to collect a small amount of data related to the question above.\n",
        "\n",
        "Data considerations:\n",
        "- How many military personnel are stationed at different military installations.?\n",
        "    - The installations with the highest number of military personnel should have higher numbers of military spouses seeking employment.\n",
        "\n",
        "- How much residential cleaning demand is in each town?\n",
        "    - Using Beautiful Soup web scraping techniques to determine the nature of residential cleaning demand around different military\n",
        "      installations.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Dictionary of top 10 military installations with the most service members and their locations:\n",
        "locations = {\n",
        "    \"Fort Liberty\": \"Fort Liberty, North Carolina\",\n",
        "    \"Joint Base San Antonio\": \"Joint Base San Antonio, Texas\",\n",
        "    \"Fort Cavazos\": \"Fort Cavazos, Texas\",\n",
        "    \"Joint Base Lewis-McChord\": \"Joint Base Lewis-McChord, Washington\",\n",
        "    \"Naval Station Norfolk\": \"Naval Station Norfolk, Virginia\",\n",
        "    \"Camp Pendleton\": \"Camp Pendleton, California\",\n",
        "    \"Fort Campbell\": \"Fort Campbell, Kentucky\",\n",
        "    \"Joint Base Elmendorf-Richardson\": \"Joint Base Elmendorf-Richardson, Alaska\",\n",
        "    \"Fort Benning\": \"Fort Benning, Georgia\",\n",
        "    \"Fort Stewart\": \"Fort Stewart, Georgia\"\n",
        "}\n",
        "\n",
        "# Function to scrape cleaning service information from Yelp\n",
        "def fetch_cleaning_data(location, num_samples=200):\n",
        "    base_url = \"https://www.yelp.com/search\"\n",
        "    response = requests.get(base_url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    businesses = soup.find_all('h4', class_='css-1l5lt1i')  # Extract business names\n",
        "    cleaning_data = []\n",
        "\n",
        "    for business in businesses[:num_samples]:\n",
        "        business_name = business.get_text\n",
        "        cleaning_data.append({\n",
        "            \"Business Name\": business_name,\n",
        "            \"Location\": location\n",
        "        })\n",
        "\n",
        "    return cleaning_data\n",
        "\n",
        "# Function to collect data for 1000 samples\n",
        "def collect_data():\n",
        "    all_data = []\n",
        "\n",
        "    for base, location in locations.items():\n",
        "        print(f\"Fetching cleaning data for {location}...\")\n",
        "\n",
        "        cleaning_services = fetch_cleaning_data(location)\n",
        "        for service in cleaning_services:\n",
        "            all_data.append({\n",
        "                'Installation': base,\n",
        "                'Location': location,\n",
        "                'Business Name': service['Business Name']\n",
        "            })\n",
        "\n",
        "        time.sleep(1) # avoiding overloading the server\n",
        "\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "df = collect_data()\n",
        "df.to_csv('cleaning_expansion_data.csv', index=False)\n",
        "\n",
        "print(\"Collected data and saved to 'cleaning_expansion_data.csv'\")"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecabc94-5260-46ee-a2a7-63beae6e6c54"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching cleaning data for Fort Liberty, North Carolina...\n",
            "Fetching cleaning data for Joint Base San Antonio, Texas...\n",
            "Fetching cleaning data for Fort Cavazos, Texas...\n",
            "Fetching cleaning data for Joint Base Lewis-McChord, Washington...\n",
            "Fetching cleaning data for Naval Station Norfolk, Virginia...\n",
            "Fetching cleaning data for Camp Pendleton, California...\n",
            "Fetching cleaning data for Fort Campbell, Kentucky...\n",
            "Fetching cleaning data for Joint Base Elmendorf-Richardson, Alaska...\n",
            "Fetching cleaning data for Fort Benning, Georgia...\n",
            "Fetching cleaning data for Fort Stewart, Georgia...\n",
            "Collected data and saved to 'cleaning_expansion_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14c26066-75db-4570-b13d-fac2fbbcaf73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scholarly in /usr/local/lib/python3.10/dist-packages (1.7.11)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.12.3)\n",
            "Requirement already satisfied: bibtexparser in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.4.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.2.14)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.5.1)\n",
            "Requirement already satisfied: free-proxy in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.1.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from scholarly) (0.27.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.0.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from scholarly) (2.32.3)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.24.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from scholarly) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->scholarly) (2.8.2)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow->scholarly) (2.9.0.20240906)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->scholarly) (2.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from bibtexparser->scholarly) (3.1.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->scholarly) (1.16.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from free-proxy->scholarly) (4.9.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (2.0.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium->scholarly) (0.26.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium->scholarly) (0.11.1)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium->scholarly) (1.8.0)\n",
            "Requirement already satisfied: sphinx<8,>=5 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (0.18.1)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (24.1)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (1.3.0.post0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium->scholarly) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.5)\n",
            "Requirement already satisfied: scholarly in /usr/local/lib/python3.10/dist-packages (1.7.11)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.12.3)\n",
            "Requirement already satisfied: bibtexparser in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.4.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.2.14)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.5.1)\n",
            "Requirement already satisfied: free-proxy in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.1.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from scholarly) (0.27.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.0.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from scholarly) (2.32.3)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.24.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from scholarly) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->scholarly) (2.8.2)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow->scholarly) (2.9.0.20240906)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->scholarly) (2.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from bibtexparser->scholarly) (3.1.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->scholarly) (1.16.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from free-proxy->scholarly) (4.9.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (2.0.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium->scholarly) (0.26.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium->scholarly) (0.11.1)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium->scholarly) (1.8.0)\n",
            "Requirement already satisfied: sphinx<8,>=5 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (0.18.1)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (24.1)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (1.3.0.post0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium->scholarly) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.5)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'scholarly' has no attribute 'search_pubs_query'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c88a9af7adae>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"XYZ\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# save to csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c88a9af7adae>\u001b[0m in \u001b[0;36mfetch_articles\u001b[0;34m(query, num_articles)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                         \u001b[0;31m# ...query and num_articles (defaulted to 1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0msearch_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscholarly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_pubs_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# searches for the query in Google Scholar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m                                       \u001b[0;31m# creates an empty list to store the articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'scholarly' has no attribute 'search_pubs_query'"
          ]
        }
      ],
      "source": [
        "# I have most of Sunday working on this problem and couldn't figure out why it wouldn't work.\n",
        "# It turns out, semanticscholar blocks scraping unless you use their API, which I applied for but haven't received a response.\n",
        "# I couldn't get it to work using Beautiful Soup (from bs4 import BeautifulSoup) and (soup = BeautifulSoup(response.text, \"html.parser\")),\n",
        "# I decided to use scholarly and scrape Google Scholarly. I ran into issues with scholarly as well. I think they blocked me. I finally gave\n",
        "# ...up on this one as well.\n",
        "\n",
        "!pip install scholarly          # installing scholarly library for scraping Google Scholarly\n",
        "!pip install --upgrade scholarly # upgrading scholarly library for scraping Google Scholarly\n",
        "import scholarly                # importing scholarly\n",
        "import pandas as pd             # importing pandas for data manipulation and analysis\n",
        "\n",
        "def fetch_articles(query, num_articles=1000):         # a new function to fetch articles; accepts two parameters;\n",
        "                                                        # ...query and num_articles (defaulted to 1000)\n",
        "\n",
        "  search_query = scholarly.search_pubs_query(query)   # searches for the query in Google Scholar\n",
        "\n",
        "  articles = []                                       # creates an empty list to store the articles\n",
        "  count = 0                                           # initializes a counter to track the number of articles\n",
        "\n",
        "  while len(articles) < num_articles:                 # a loop until  the number of articles is collected\n",
        "    try:\n",
        "      paper = next(search_query)                      # get the next paper from the search results\n",
        "\n",
        "      # finds the title or \"N/A\" if missing\n",
        "      title = paper.bib.get('title', 'N/A')\n",
        "\n",
        "      # finds the venue/journal or \"N/A\" if missing\n",
        "      venue = paper.bib.get('journal', 'N/A')\n",
        "\n",
        "      # finds the publication year, or \"N/A\" if missing\n",
        "      year = paper.bib.get(\"year\", \"N/A\")\n",
        "\n",
        "      # finds the authors, or \"N/A\" if missing\n",
        "      authors = \", \".join(paper.bib.get(\"author\", []))\n",
        "\n",
        "      # finds the abstract, or \"N/A\" if missing\n",
        "      abstract = paper.bib.get(\"abstract\", \"N/A\")\n",
        "\n",
        "      # adds a dictionary with the article details to the articles list\n",
        "      articles.append({\n",
        "          \"Title\": title,\n",
        "          \"Venue\": venue,\n",
        "          \"Year\": year,\n",
        "          \"Authors\": authors,\n",
        "          \"Abstract\": abstract\n",
        "      })\n",
        "\n",
        "      count += 1                # increments the counter\n",
        "      if count >= num_articles: # breaks the loop once you reach the desired num of articles\n",
        "        break\n",
        "\n",
        "    except StopIteration:       # if no more articles are found, break the loop\n",
        "      break\n",
        "\n",
        "  return articles               # returns the list of articles\n",
        "\n",
        "# example\n",
        "query = \"XYZ\"\n",
        "articles = fetch_articles(query, num_articles)\n",
        "\n",
        "# save to csv file\n",
        "df = pd.DataFrame(articles)\n",
        "df.to_csv(\"articles.csv\", index=False)\n",
        "\n",
        "print(f\"Collected {len(articles)} articles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e6a7c1-6974-4e7f-b8f1-ed73bb8d3107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Received status code 403\n",
            "No data was collected.\n"
          ]
        }
      ],
      "source": [
        "# I got close with this one, but I couldn't figure out why I was getting the 403 error. Maybe I'm being blocked by Reddit.\n",
        "\n",
        "import requests                 # Import the requests library to make HTTP requests\n",
        "import pandas as pd             # Import pandas for handling and analyzing data\n",
        "\n",
        "# Function to fetch Reddit data using Pushshift API\n",
        "def fetch_reddit_data(keyword, num_posts=100):\n",
        "    base_url = 'https://api.pushshift.io/reddit/search/submission/'  # Base URL for the Pushshift API\n",
        "    params = {                    # Parameters for the API request\n",
        "        'q': keyword,             # Keyword to search in Reddit posts\n",
        "        'size': num_posts,        # Number of posts to retrieve\n",
        "        'sort': 'desc',           # Sort by newest posts first\n",
        "        'sort_type': 'created_utc' # Sort based on post creation time\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)   # Make the API request\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: Received status code {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    # Check if the 'data' field is present in the response\n",
        "    try:\n",
        "        data = response.json()['data']    # Extract the 'data' field from the JSON response\n",
        "    except KeyError:\n",
        "        print(\"Error: 'data' field not found in the response.\")\n",
        "        print(\"Full response:\", response.json())  # Print full response for debugging\n",
        "        return None\n",
        "\n",
        "    posts_data = []             # List to hold the collected posts data\n",
        "\n",
        "    for post in data:           # Loop through each post in the data\n",
        "        posts_data.append({\n",
        "            'Title': post.get('title', 'N/A'),            # Extract post title or use 'N/A' if not available\n",
        "            'Subreddit': post.get('subreddit', 'N/A'),    # Extract subreddit name or use 'N/A'\n",
        "            'Username': post.get('author', 'N/A'),        # Extract author's username or use 'N/A'\n",
        "            'Upvotes': post.get('score', 'N/A'),          # Extract upvote count or use 'N/A'\n",
        "            'Created_UTC': post.get('created_utc', 'N/A') # Extract timestamp of post creation or use 'N/A'\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(posts_data)  # Create a DataFrame from the list of posts\n",
        "    return df                      # Return the DataFrame\n",
        "\n",
        "# Test the function by searching for the keyword 'python' and retrieving 100 posts\n",
        "keyword = 'python'\n",
        "reddit_df = fetch_reddit_data(keyword, 100)\n",
        "\n",
        "# Check if data was returned before saving\n",
        "if reddit_df is not None:\n",
        "    # Save the collected data to a CSV file\n",
        "    reddit_df.to_csv('reddit_data.csv', index=False)     # Save the DataFrame to a CSV file\n",
        "    print('Collected Reddit data and saved it to \"reddit_data.csv\"')\n",
        "else:\n",
        "    print(\"No data was collected.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I spent most or Saturday and Sunday trying to understand what I was getting wrong. I went through many versions of these.\n",
        "I think we should extend the due date on these assignments until the end of Monday. I can't come see the TA on Thursday or\n",
        "Friday because of either my personal schedule or class schedules.\n",
        "I work on this stuff over the weekend and if we had Monday to discuss with them I could probably solve these errors before submitting.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}