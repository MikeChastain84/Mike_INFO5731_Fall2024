{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeChastain84/Mike_INFO5731_Fall2024/blob/main/Chastain_Mike_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "fb2ef432-ea65-4192-e6da-0c2f3e8e07cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\n\\nSpam detection would be an interesting text classification model.\\n\\nFeatures:\\n1. Bag of Words. Spam messages like to create a sense of urgency or provide an incintive to encourage a user to take an action \\n    such as respond to a message or click a link. By counting the word occurances for words commonly used to create a sense of urgency \\n    or incintivize a user (prize, win, \"must act now,\" etc.) you can begin to determine the likelihood of a message being spam.\\n\\n2. Presence of links, URL, and especially short URLs. A feature that checks for the presence of URLs would be affective.\\n\\n3. TF-IDF could be used to identify the importance of words in a message. When multiple words commonly use in spam (offer, win, urgent) are \\n    identified in the message, it\\'s likely to be spam.\\n\\n4. Linguistic features can help an ML model identify identify language patterns common to spam. Spam messages try to entice users to take an\\n    action phrases such as \"act now,\" \"take action,\" \"limited time,\" etc. Linguistic features can help identify these language patterns.\\n\\n5. Message length. Spam messages are usally around the same length. They\\'re usually short and to the point or lengthy. These could be used \\n    to flag a message as potential spam. \\n\\n6. Unknown sender. I don\\'t know how, but if you can check the sender against the receivers known contact this could prove useful. Spam\\n    messages usually originate from an unknown sender.\\n\\n7. Poor grammar. Spam messages often contain poor grammar combined with a sense of urgency. Being able to detect poor grammar could be helpful.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Spam detection would be an interesting text classification model.\n",
        "\n",
        "Features:\n",
        "1. Bag of Words. Spam messages like to create a sense of urgency or provide an incintive to encourage a user to take an action\n",
        "    such as respond to a message or click a link. By counting the word occurances for words commonly used to create a sense of urgency\n",
        "    or incintivize a user (prize, win, \"must act now,\" etc.) you can begin to determine the likelihood of a message being spam.\n",
        "\n",
        "2. Presence of links, URL, and especially short URLs. A feature that checks for the presence of URLs would be affective.\n",
        "\n",
        "3. TF-IDF could be used to identify the importance of words in a message. When multiple words commonly use in spam (offer, win, urgent) are\n",
        "    identified in the message, it's likely to be spam.\n",
        "\n",
        "4. Linguistic features can help an ML model identify identify language patterns common to spam. Spam messages try to entice users to take an\n",
        "    action phrases such as \"act now,\" \"take action,\" \"limited time,\" etc. Linguistic features can help identify these language patterns.\n",
        "\n",
        "5. Message length. Spam messages are usally around the same length. They're usually short and to the point or lengthy. These could be used\n",
        "    to flag a message as potential spam.\n",
        "\n",
        "6. Unknown sender. I don't know how, but if you can check the sender against the receivers known contact this could prove useful. Spam\n",
        "    messages usually originate from an unknown sender.\n",
        "\n",
        "7. Poor grammar. Spam messages often contain poor grammar combined with a sense of urgency. Being able to detect poor grammar could be helpful.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ce0578-b74b-45b9-cbd8-84ee00075a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Bag of Words Results:\n",
            "vocabulary {'win': 19, 'free': 4, 'ipad': 8, 'click': 2, 'this': 15, 'link': 10, 'limited': 9, 'time': 16, 'offer': 12, 'here': 6, 'to': 17, 'claim': 1, 'your': 21, 'prize': 13, 'hello': 5, 'are': 0, 'you': 20, 'there': 14, 'hurry': 7, 'now': 11, 'what': 18, 'doing': 3}\n",
            "shape (5, 22)\n",
            "vectors:  [[0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0]\n",
            " [0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1]\n",
            " [1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
            " [0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1]\n",
            " [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]]\n",
            "\n",
            "2. Presence of links or URLs Results:\n",
            "                                             Message  URL\n",
            "0                  Win a free ipad. Click this link.    0\n",
            "1  Limited time offer! Click here to claim your p...    0\n",
            "2                              Hello, are you there?    0\n",
            "3                     Hurry now to claim your prize!    0\n",
            "4                                What are you doing?    0\n",
            "\n",
            "3. TF-IDF Results:\n",
            "        are   are you     claim  claim your     click  click here  click this  \\\n",
            "0  0.000000  0.000000  0.000000    0.000000  0.247212    0.000000    0.306413   \n",
            "1  0.000000  0.000000  0.214046    0.214046  0.214046    0.265304    0.000000   \n",
            "2  0.330677  0.330677  0.000000    0.000000  0.000000    0.000000    0.000000   \n",
            "3  0.000000  0.000000  0.275814    0.275814  0.000000    0.000000    0.000000   \n",
            "4  0.330677  0.330677  0.000000    0.000000  0.000000    0.000000    0.000000   \n",
            "\n",
            "      doing      free  free ipad  ...  to claim      what  what are       win  \\\n",
            "0  0.000000  0.306413   0.306413  ...  0.000000  0.000000  0.000000  0.306413   \n",
            "1  0.000000  0.000000   0.000000  ...  0.214046  0.000000  0.000000  0.000000   \n",
            "2  0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "3  0.000000  0.000000   0.000000  ...  0.275814  0.000000  0.000000  0.000000   \n",
            "4  0.409865  0.000000   0.000000  ...  0.000000  0.409865  0.409865  0.000000   \n",
            "\n",
            "   win free       you  you doing  you there      your  your prize  \n",
            "0  0.306413  0.000000   0.000000   0.000000  0.000000    0.000000  \n",
            "1  0.000000  0.000000   0.000000   0.000000  0.214046    0.214046  \n",
            "2  0.000000  0.330677   0.000000   0.409865  0.000000    0.000000  \n",
            "3  0.000000  0.000000   0.000000   0.000000  0.275814    0.275814  \n",
            "4  0.000000  0.330677   0.409865   0.000000  0.000000    0.000000  \n",
            "\n",
            "[5 rows x 42 columns]\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "# 1. Bag of Words (BOW):\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Create messages to serve as test data:\n",
        "messages = [\n",
        "    \"Win a free ipad. Click this link.\",                    # Spam\n",
        "    \"Limited time offer! Click here to claim your prize.\",  # Spam\n",
        "    \"Hello, are you there?\",                                # Not spam\n",
        "    \"Hurry now to claim your prize!\",                       # Spam\n",
        "    \"What are you doing?\"                                   # Not spam\n",
        "]\n",
        "\n",
        "# Bag of words: Initialize the object vect. The first instance of the tool, CountVectorizer().\n",
        "vect = CountVectorizer()\n",
        "\n",
        "# Get counts of each token in the text data. Store this as a sparse array in x.\n",
        "x = vect.fit_transform(messages)\n",
        "\n",
        "# Converts sparse array to a dense array\n",
        "x.toarray()\n",
        "\n",
        "# view token vocabulary and counts\n",
        "print(\"1. Bag of Words Results:\")\n",
        "print(\"vocabulary\", vect.vocabulary_)\n",
        "print(\"shape\", x.shape)\n",
        "print(\"vectors: \", x.toarray())\n",
        "\n",
        "# \"\"\"\n",
        "# Common spam words: win, free, ipad, click, link, limited, time, offer, claim, prize, hurry\n",
        "\n",
        "# From my output:\n",
        "\n",
        "# First message ([0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0]):\n",
        "# Contains 4 common spam words: \"win\", \"free\", \"click\", \"link\"\n",
        "# These words are strong indicators of spam. This message is likely spam.\n",
        "\n",
        "# Second message ([0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1]):\n",
        "# Contains 5 common spam words: \"claim\", \"click\", \"prize\", \"limited\", \"time\"\n",
        "# These words are strong indicators of spam. This message is likely spam.\n",
        "\n",
        "# Third message ([1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]):\n",
        "# Contains: \"hello\", \"are\", \"there\", \"you\"\n",
        "# These are words are found in regular conversations. This message not likely to be spam.\n",
        "\n",
        "# Fourth message ([0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1]):\n",
        "# Contains 4 common spam words: \"claim\", \"prize\", \"hurry\", \"now\"\n",
        "# These words are strong indicators of spam. This message is likely spam.\n",
        "\n",
        "# Fifth message ([1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]):\n",
        "# Contains: \"what\", \"doing\", \"you\"\n",
        "# These are words are found in regular conversations. This message not likely to be spam.\n",
        "# \"\"\"\n",
        "\n",
        "# 2. Presence of links or URLs:\n",
        "import re\n",
        "\n",
        "# define a function to detect URLs using regex:\n",
        "def contains_url(text):\n",
        "  url_pattern = r'(https?://\\S+|www\\.\\S+)'          # Regular expression to match URLs\n",
        "  return 1 if re.search(url_pattern, text) else 0   # 1 - URL present; 0 - no URL\n",
        "\n",
        "# apply contains_url() function to messages\n",
        "url_feature = [contains_url(message) for message in messages]\n",
        "\n",
        "# create dataframe to display with URL feature\n",
        "df = pd.DataFrame({'Message': messages, 'URL': url_feature})\n",
        "\n",
        "# display dataframe\n",
        "print(\"\\n2. Presence of links or URLs Results:\")\n",
        "print(df)\n",
        "\n",
        "# 3. TF-IDF:\n",
        "\n",
        "# Import Libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer   # Imports the TF-IDF Vectorizer class\n",
        "\n",
        "# Create an instance (object) of the TfidfVectorizer class. This will be used to convert the text data into a matrix of TF-IDF scores.\n",
        "TFIDF = TfidfVectorizer(min_df=1, ngram_range=(1, 2))\n",
        "    # min_df=1 - include any word (or term) that appears in at least one document\n",
        "    # ngram_range=(1, 2) - consider both single words (1-grams) and pairs of consecutive words (2-grams) as features\n",
        "\n",
        "# Convert the text data into a sparse matrix of TF-IDF scores. Each row is a document. Each column is a word or word pair.\n",
        "# ...The values in the matrix are the TF-IDF scores for those words in each document.\n",
        "features = TFIDF.fit_transform(messages)\n",
        "\n",
        "# Converts sparse matrix data to dense matrix and saves it as \"df_tfidf\"\n",
        "df_tfidf = pd.DataFrame(\n",
        "    features.todense(),                   # applies todense() to the features matrix converting it to a dense matrix\n",
        "    columns=TFIDF.get_feature_names_out() # gets the list of words (n-grams) and uses them to label the columns\n",
        "    )\n",
        "\n",
        "# Display the TF-IDF matrix\n",
        "print(\"\\n3. TF-IDF Results:\")\n",
        "print(df_tfidf)\n",
        "\n",
        "\n",
        "# Common spam words: win, free, ipad, click, link, limited, time, offer, claim, prize, hurry\n",
        "\n",
        "# From the 3.TF-IDF Results below:\n",
        "\n",
        "# Each row represents a message.\n",
        "# 0: \"Win a free iPad. Click this link.\" (Spam)\n",
        "# 1: \"Limited time offer! Click here to claim your prize.\" (Spam)\n",
        "# 2: \"Hello, are you there?\" (Not spam)\n",
        "# 3: \"Hurry now to claim your prize!\" (Spam)\n",
        "# 4: \"What are you doing?\" (Not spam)\n",
        "\n",
        "# Each column represents a word or word pair as labeled by the TF-IDF Vectorizer.\n",
        "# The values in the matrix are the TF-IDF scores for those words in each message.\n",
        "\n",
        "# The messages with high scores aligned with the common spam words list are 0, 1, and 3.\n",
        "# These messages are likely to be spam.\n",
        "\n",
        "# The messages with low scores aligned with the common spam words list are 2 and 4.\n",
        "# These messages are not likely to be spam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94167e15-03c5-4682-970d-3ab1c987499d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chi-square Results:\n",
            "    Feature  Chi-square  p-value\n",
            "9   limited         0.5   0.4795\n",
            "17     time         0.5   0.4795\n",
            "14     rare         0.5   0.4795\n",
            "4      free         0.5   0.4795\n",
            "12    offer         0.5   0.4795\n",
            "6      here         0.5   0.4795\n",
            "7     hurry         0.5   0.4795\n",
            "8      ipad         0.5   0.4795\n",
            "11      now         0.5   0.4795\n",
            "20      win         0.5   0.4795\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "# First, import necessary libraries and tools:\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import chi2                  # I selected chi-squared test\n",
        "import pandas as pd\n",
        "\n",
        "# Next, create messages for test data and labels (spam = 1, not spam = 0)\n",
        "messages = [\n",
        "    \"Win a free ipad. Click this link.\",                    # Spam\n",
        "    \"Limited time offer! Click here to claim prize.\",       # Spam\n",
        "    \"Hello, are you there?\",                                # Not spam\n",
        "    \"Hurry now to claim prize!\",                            # Spam\n",
        "    \"Click this link to claim a rare prize!\",               # Spam\n",
        "    \"What are you doing?\"                                   # Not spam\n",
        "]\n",
        "\n",
        "# Create labels:\n",
        "labels = [1, 1, 0, 1, 1, 0]                                    # spam = 1, not spam = 0\n",
        "\n",
        "# Vectorize the messages using CountVectorizer():\n",
        "vect = CountVectorizer()\n",
        "x = vect.fit_transform(messages)\n",
        "\n",
        "# Apply the chi-squared test to the word counts and labels:\n",
        "chi2_scores, p_values = chi2(x, labels)\n",
        "\n",
        "# Create a data frame using pandas with feature names, Chi-square scores, and p-values:\n",
        "feature_names = vect.get_feature_names_out()\n",
        "chi2_df = pd.DataFrame({'Feature': feature_names,   # column for feature names (words)\n",
        "                        'Chi-square': chi2_scores,  # column for Chi-square scores\n",
        "                        'p-value': p_values})       # column for p-values\n",
        "\n",
        "# Sort in descending order by Chi-square score:\n",
        "chi2_df_sorted = chi2_df.sort_values(by='Chi-square', ascending=True)\n",
        "\n",
        "# Display the results:\n",
        "print(\"Chi-square Results:\")\n",
        "print(chi2_df_sorted.head(10))  # print the top 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e058ac1-b683-4c98-d0f8-8b51a9930733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages ranked by similarity to the query:\n",
            "1. Hurry now to claim prize! (Similarity: 0.7788)\n",
            "2. Limited time offer! Click here to claim prize. (Similarity: 0.6713)\n",
            "3. Click this link to claim a rare prize! (Similarity: 0.6499)\n",
            "4. Win a free ipad. Click this link. (Similarity: 0.5913)\n",
            "5. What are you doing? (Similarity: 0.5332)\n",
            "6. Hello, are you there? (Similarity: 0.5086)\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "# Import necessary libraries to work with BERT and cosine similarity\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "# We will use 'bert-base-uncased', a common pre-trained BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a function to get BERT embeddings (vector representation) for a piece of text\n",
        "# This function tokenizes the input text and gets the BERT output embeddings\n",
        "# We use 'torch.no_grad()' to avoid calculating gradients since we are only using the model for inference\n",
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)  # Tokenize input text\n",
        "    with torch.no_grad():  # We don't need to calculate gradients when using a pre-trained model for embeddings\n",
        "        outputs = model(**inputs)  # Get the model output\n",
        "    # 'last_hidden_state' is the final output of the BERT model\n",
        "    # We take the mean of the output vectors along the sequence dimension to get a single vector for the entire text\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
        "\n",
        "# Prepare the text data (messages) that we want to rank by similarity to a query\n",
        "messages = [\n",
        "    \"Win a free ipad. Click this link.\",\n",
        "    \"Limited time offer! Click here to claim prize.\",\n",
        "    \"Hello, are you there?\",\n",
        "    \"Hurry now to claim prize!\",\n",
        "    \"Click this link to claim a rare prize!\",\n",
        "    \"What are you doing?\"\n",
        "]\n",
        "\n",
        "# 1 = Spam, 0 = Not spam\n",
        "labels = [1, 1, 0, 1, 1, 0]\n",
        "\n",
        "# The query that we want to match the most relevant messages to\n",
        "query = \"Claim your free prize now\"\n",
        "\n",
        "# Get BERT embeddings for each of the messages in the list\n",
        "# We use a list comprehension to iterate over each message and convert it to its BERT embedding\n",
        "message_embeddings = [get_bert_embedding(message) for message in messages]\n",
        "\n",
        "# Get the BERT embedding for the query\n",
        "# We will compare this embedding with the message embeddings to find the most similar ones\n",
        "query_embedding = get_bert_embedding(query)\n",
        "\n",
        "# Calculate the cosine similarity between the query and each message\n",
        "# Cosine similarity measures the similarity between two vectors by calculating the cosine of the angle between them\n",
        "# The closer the cosine similarity is to 1, the more similar the vectors are\n",
        "cosine_scores = [cosine_similarity(query_embedding.unsqueeze(0), message.unsqueeze(0)).item() for message in message_embeddings]\n",
        "\n",
        "# Sort the messages by their similarity score in descending order (most similar first)\n",
        "# We zip the messages and their corresponding similarity scores together, and sort them by the score\n",
        "sorted_messages = sorted(zip(messages, cosine_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the sorted messages along with their similarity scores\n",
        "# We enumerate over the sorted messages to print them in a numbered list\n",
        "print(\"Messages ranked by similarity to the query:\")\n",
        "for i, (message, score) in enumerate(sorted_messages, start=1):\n",
        "    print(f\"{i}. {message} (Similarity: {score:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "I feel like I learned a tremendous amount in this module; more than in previous modules. I wonder if that is because I'm more familiar\n",
        "with the subject or if the presentation was different. Either way, I found this to be a very interesting module.\n",
        "\n",
        "The demonstration by Ms. Fengjiao was to the point and very informative. It was easy for me to follow and replicate when the time came.\n",
        "I was confident implimenting these methods quickly in this exercise on questions 1 - 3.\n",
        "\n",
        "Question 4 was the exception. That question was very difficult. I don't feel like it aligned to much of our discussion in class. This\n",
        "question required a lot of research for me. I guess that is good and bad. I didn't realize it would take as much time as it did. I had\n",
        "to use stackoverflow to understand which libraries to import and then it took me a long time to find a way to write the code. I still\n",
        "never figured out how to get a token from huggingface.com. I did however figure out how to suppress the warning using:\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "The code seems to work correctly so I'm happy with it.\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "fee796dc-6f61-4127-9059-a588243f6f9b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\n\\nI feel like I learned a tremendous amount in this module; more than in previous modules. I wonder if that is because I\\'m more familiar\\nwith the subject or if the presentation was different. Either way, I found this to be a very interesting module.\\n\\nThe demonstration by Ms. Fengjiao was to the point and very informative. It was easy for me to follow and replicate when the time came.\\nI was confident implimenting these methods quickly in this exercise on questions 1 - 3.\\n\\nQuestion 4 was the exception. That question was very difficult. I don\\'t feel like it aligned to much of our discussion in class. This\\nquestion required a lot of research for me. I guess that is good and bad. I didn\\'t realize it would take as much time as it did. I had \\nto use stackoverflow to understand which libraries to import and then it took me a long time to find a way to write the code. I still\\nnever figured out how to get a token from huggingface.com. I did however figure out how to suppress the warning using:\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")\\nThe code seems to work correctly so I\\'m happy with it.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}